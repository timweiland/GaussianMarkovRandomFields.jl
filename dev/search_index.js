var documenterSearchIndex = {"docs":
[{"location":"tutorials/spatiotemporal_modelling/#Spatiotemporal-Modelling-with-SPDEs","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"","category":"section"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"In this tutorial, we will demonstrate how to perform spatiotemporal Bayesian inference using GMRFs based on a simple toy example.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/#Problem-setup","page":"Spatiotemporal Modelling with SPDEs","title":"Problem setup","text":"","category":"section"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Our goal is to model how a pollutant (i.e. some chemical) spreads across a river over time. We get (noisy) measurements of the pollutant concentration across the domain at time t = 0, and an additional measurement at some later point in time. To simplify things, we model the river as a 1D domain. Let's set up the problem:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"x_left, x_right = -1.0, 1.0\nNₓ = 201\nt_start, t_stop = 0.0, 1.0\nNₜ = 71\nts = range(t_start, t_stop, length = Nₜ)\nf_initial = x -> exp(-(x + 0.6)^2 / 0.2^2)\nxs_initial = range(x_left, x_right, length = Nₓ ÷ 2)\nys_initial = f_initial.(xs_initial)\nnoise_precision_initial = 0.1^(-2)\n\nx_later = -0.25\ny_later = 0.55\nnoise_precision_later = 0.01^(-2)\n\nxs_all = [xs_initial; x_later]\nys_all = [ys_initial; y_later]\nN_obs_all = length(ys_all)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/#Using-GMRFs-for-spatiotemporal-modelling","page":"Spatiotemporal Modelling with SPDEs","title":"Using GMRFs for spatiotemporal modelling","text":"","category":"section"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Fundamentally, we are interested in inferring a spatiotemporal function that models the pollutant concentration over time, which is an infinite-dimensional object. GMRFs, however, are finite-dimensional. This is not a limitation, it just means that we need to discretize space and time to ultimately obtain a discrete approximation to the infinite-dimensional object.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"We're going to do this as follows:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Set up a stochastic PDE (SPDE) that models the spatiotemporal, infinite-dimensional prior (a Gaussian process).\nDiscretize the SPDE in space and time to obtain a GMRF which approximates the Gaussian process.\nCondition the GMRF on the observations to obtain the posterior.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Let's start by setting up our discretizations:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"using GaussianMarkovRandomFields\nusing Ferrite\n\ngrid = generate_grid(Line, (Nₓ - 1,), Tensors.Vec(x_left), Tensors.Vec(x_right))\ninterpolation = Lagrange{RefLine, 1}()\nquadrature_rule = QuadratureRule{RefLine}(2)\ndisc = FEMDiscretization(grid, interpolation, quadrature_rule)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/#A-separable-model","page":"Spatiotemporal Modelling with SPDEs","title":"A separable model","text":"","category":"section"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Perhaps the simplest spatiotemporal model is a separable one, where the spatial and temporal components are independent. We can model both the spatial and temporal components using Matern processes:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"spde_space = MaternSPDE{1}(range = 0.2, smoothness = 1, σ² = 0.3)\nspde_time = MaternSPDE{1}(range = 0.5, smoothness = 1)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Discretize:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"x_space = discretize(spde_space, disc)\nQ_s = precision_map(x_space)\n\ngrid_time = generate_grid(Line, (Nₜ - 1,), Tensors.Vec(t_start), Tensors.Vec(t_stop))\ndisc_time = FEMDiscretization(grid_time, interpolation, quadrature_rule)\nx_time = discretize(spde_time, disc_time)\nQ_t = precision_map(x_time)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Create the separable spatiotemporal model:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"x_st_kron = kronecker_product_spatiotemporal_model(Q_t, Q_s, disc)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Great! Now let's condition on the observations. To do this, we construct a \"spatial\" observation matrix and transform it into a \"spatiotemporal\" observation matrix:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"A_initial = evaluation_matrix(disc, [Tensors.Vec(x) for x in xs_initial])\nt_initial_idx = 1 # Observe at first time point\nA_initial = spatial_to_spatiotemporal(A_initial, t_initial_idx, Nₜ)\nA_later = evaluation_matrix(disc, [Tensors.Vec(x_later)])\nt_later_idx = 2 * Nₜ ÷ 3\nA_later = spatial_to_spatiotemporal(A_later, t_later_idx, Nₜ)\n\nA_all = [A_initial; A_later]\n\nusing LinearAlgebra, SparseArrays\nQ_noise = sparse(I, N_obs_all, N_obs_all) * noise_precision_initial\nQ_noise[end, end] = noise_precision_later","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Condition on the observations:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"x_st_kron_posterior = condition_on_observations(x_st_kron, A_all, Q_noise, ys_all)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Let's look at the dynamics of this posterior.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"using CairoMakie\nCairoMakie.activate!()\nplot(x_st_kron_posterior, t_initial_idx)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"plot(x_st_kron_posterior, Nₜ ÷ 3)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"plot(x_st_kron_posterior, 2 * Nₜ ÷ 3)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"plot(x_st_kron_posterior, Nₜ)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"We see that the effect of our observations effectively just \"dies off\" over time. And generally in spatiotemporal statistics, this is a fair assumption: The further away in time we are from an observation, the less it should influence our predictions.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"But in our case, we know a bit more about the phenomenon at hand. This is a river, and we know that it flows in a certain direction, and we probably also roughly know the flow speed. We should embed this information into our prior to get a more useful posterior!","category":"page"},{"location":"tutorials/spatiotemporal_modelling/#Advection-diffusion-priors","page":"Spatiotemporal Modelling with SPDEs","title":"Advection-diffusion priors","text":"","category":"section"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"We can achieve this through a non-separable model that encodes these dynamics. Concretely, we are going to consider an advection-diffusion SPDE as presented in [2].","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"adv_diff_spde = AdvectionDiffusionSPDE{1}(\n    γ = [-0.6],\n    H = 0.1 * sparse(I, (1, 1)),\n    τ = 0.1,\n    α = 2 // 1,\n    spatial_spde = spde_space,\n    initial_spde = spde_space,\n)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"To discretize this SPDE, we only need a FEM discretization of space. For time, we simply specify the discrete time points which are then used internally for an implicit Euler scheme.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"x_adv_diff = discretize(adv_diff_spde, disc, ts)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Condition on the initial observations:","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"x_adv_diff_posterior = condition_on_observations(x_adv_diff, A_all, Q_noise, ys_all)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"Let's look at the dynamics of this posterior.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"plot(x_adv_diff_posterior, t_initial_idx)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"plot(x_adv_diff_posterior, Nₜ ÷ 3)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"plot(x_adv_diff_posterior, 2 * Nₜ ÷ 3)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"plot(x_adv_diff_posterior, Nₜ)","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"This looks much more reasonable! We see that the pollutant is transported downstream over time, and the observations at the later time point are consistent with this.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/#Conclusion","page":"Spatiotemporal Modelling with SPDEs","title":"Conclusion","text":"","category":"section"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"We have seen how to model spatiotemporal phenomena using GMRFs. We started with a simple separable model and then moved on to a more complex advection-diffusion model. The latter model was able to capture the dynamics of the river and the pollutant transport much better.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"As mentioned initially, this was a simple toy example. But the underlying principles are the same for more complex problems. In particular, all of the above should work the same for arbitrary spatial meshes, as demonstrated e.g. in the tutorial Spatial Modelling with SPDEs.","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"","category":"page"},{"location":"tutorials/spatiotemporal_modelling/","page":"Spatiotemporal Modelling with SPDEs","title":"Spatiotemporal Modelling with SPDEs","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/boundary_conditions/#Boundary-Conditions-for-SPDEs","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"","category":"section"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"In previous tutorials, we saw that we can approximate stochastic processes with GMRFs by discretizing stochastic partial differential equations (SPDEs). Such discretizations always involve boundary conditions, which specify the behavior of the process at the boundary of the domain. In the context of the finite element method, the simplest boundary condition is a homogeneous von Neumann boundary condition, which specifies that the derivative of the process normal to the boundary is zero. This approach is also used in the seminal work by Lindgren et al. [1].","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"Yet, in practice, this behavior is often not desirable. To circumvent boundary effects, people often artificially inflate the domain by adding a buffer zone around the data.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"But what if we know the behavior of the process at the boundary, and it's not \"the normal derivative is zero\"? Fortunately, GaussianMarkovRandomFields.jl interfaces rather smoothly with Ferrite.jl, which we can use to specify more complex boundary conditions. Let's see how this works.","category":"page"},{"location":"tutorials/boundary_conditions/#Spatial-example:-Matern-SPDE","page":"Boundary Conditions for SPDEs","title":"Spatial example: Matern SPDE","text":"","category":"section"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"We start by specifying a mesh over the interval [-1, 1].","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"using GaussianMarkovRandomFields, Ferrite\ngrid = generate_grid(Line, (50,))\ninterpolation = Lagrange{RefLine, 1}()\nquadrature_rule = QuadratureRule{RefLine}(2)","category":"page"},{"location":"tutorials/boundary_conditions/#Dirichlet-boundary","page":"Boundary Conditions for SPDEs","title":"Dirichlet boundary","text":"","category":"section"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"Let us now use Ferrite to define a homogeneous Dirichlet boundary condition, which specifies that the process is zero at the boundary.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"function get_dirichlet_constraint(grid::Ferrite.Grid{1})\n    boundary = getfacetset(grid, \"left\") ∪ getfacetset(grid, \"right\")\n\n    return Dirichlet(:u, boundary, x -> (x[1] ≈ -1.0) ? 0.0 : (x[1] ≈ 1.0) ? 0.0 : 0.0)\nend\n\ndbc = get_dirichlet_constraint(grid)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"GaussianMarkovRandomFields.jl supports such boundary conditions in a \"soft\" way. This means that we enforce the boundary conditions up to noise of a certain, user-specified magnitude. This ensures that the resulting GMRF has full rank. If you don't care much for probabilistic boundary conditions, you can just set the noise to a sufficiently small value.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"bcs = [(dbc, 1.0e-4)] # 1e-4 is the noise in terms of the standard deviation","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"We can now create a FEM discretization with the specified boundary conditions.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"disc = FEMDiscretization(grid, interpolation, quadrature_rule, [(:u, nothing)], bcs)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"Let's now define some Matern SPDE and discretize it.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"matern_spde = MaternSPDE{1}(range = 0.5, smoothness = 1, σ² = 0.3)\nx = discretize(matern_spde, disc)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"Verify for yourself that the boundary value is zero for all samples, and that the variance vanishes at the boundary:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"using CairoMakie\nCairoMakie.activate!()\nplot(x, disc)","category":"page"},{"location":"tutorials/boundary_conditions/#Periodic-boundary","page":"Boundary Conditions for SPDEs","title":"Periodic boundary","text":"","category":"section"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"We can also define a periodic boundary condition in terms of an affine constraint:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"function get_periodic_constraint(grid::Ferrite.Grid{1})\n    cellidx_left, dofidx_left = collect(grid.facetsets[\"left\"])[1]\n    cellidx_right, dofidx_right = collect(grid.facetsets[\"right\"])[1]\n\n    temp_dh = DofHandler(grid)\n    add!(temp_dh, :u, Lagrange{RefLine, 1}())\n    close!(temp_dh)\n    cc = CellCache(temp_dh)\n    get_dof(cell_idx, dof_idx) = (reinit!(cc, cell_idx); celldofs(cc)[dof_idx])\n    dof_left = get_dof(cellidx_left, dofidx_left)\n    dof_right = get_dof(cellidx_right, dofidx_right)\n\n    return AffineConstraint(dof_left, [dof_right => 1.0], 0.0)\nend\n\npbc = get_periodic_constraint(grid)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"The rest of the procedure is analogous to the Dirichlet case:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"bcs = [(pbc, 1.0e-4)]\ndisc_periodic =\n    FEMDiscretization(grid, interpolation, quadrature_rule, [(:u, nothing)], bcs)\nx_periodic = discretize(matern_spde, disc_periodic)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"Verify for yourself that the values at the left and right boundary match for all samples:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"plot(x_periodic, disc)","category":"page"},{"location":"tutorials/boundary_conditions/#Spatiotemporal-example:-Advection-Diffusion-SPDE","page":"Boundary Conditions for SPDEs","title":"Spatiotemporal example: Advection-Diffusion SPDE","text":"","category":"section"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"This works just as well in the spatiotemporal case. Let's reuse our previous discretization for a 1D advection-diffusion SPDE:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"using LinearAlgebra, SparseArrays\nspde = AdvectionDiffusionSPDE{1}(\n    γ = [-0.6],\n    H = 0.1 * sparse(I, (1, 1)),\n    τ = 0.1,\n    α = 2 // 1,\n    spatial_spde = matern_spde,\n    initial_spde = matern_spde,\n)\nts = 0:0.05:1\nN_t = length(ts)\nx_adv_diff_dirichlet = discretize(spde, disc, ts)\nx_adv_diff_periodic = discretize(spde, disc_periodic, ts)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"To make things clearer, we are going to condition these GMRFs on a Gaussian initial condition to see how it propagates over time.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"xs_ic = -0.99:0.01:0.99\nys_ic = exp.(-xs_ic .^ 2 / 0.2^2)\nA_ic = evaluation_matrix(disc, [Tensors.Vec(x) for x in xs_ic])\nA_ic = spatial_to_spatiotemporal(A_ic, 1, N_t)\n\nx_adv_diff_dirichlet = condition_on_observations(x_adv_diff_dirichlet, A_ic, 1.0e8, ys_ic)\nx_adv_diff_periodic = condition_on_observations(x_adv_diff_periodic, A_ic, 1.0e8, ys_ic)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"First, check the initial observations:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"plot(x_adv_diff_dirichlet, 1)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"Now, let's see how the process evolves over time:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"plot(x_adv_diff_dirichlet, N_t ÷ 2)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"plot(x_adv_diff_dirichlet, N_t)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"Compare to this to the periodic case:","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"plot(x_adv_diff_periodic, N_t ÷ 2)","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"plot(x_adv_diff_periodic, N_t)","category":"page"},{"location":"tutorials/boundary_conditions/#Conclusion","page":"Boundary Conditions for SPDEs","title":"Conclusion","text":"","category":"section"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"We have seen how to specify more complex boundary conditions for GMRFs. All it takes is to construct the boundary conditions in Ferrite and add some noise.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"For the sake of simplicity, this tutorial considered discretizations of a one-dimensional interval, but of course the same principles apply to higher dimensions and more complex geometries.","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"","category":"page"},{"location":"tutorials/boundary_conditions/","page":"Boundary Conditions for SPDEs","title":"Boundary Conditions for SPDEs","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/preconditioners/#Preconditioners","page":"Preconditioners","title":"Preconditioners","text":"","category":"section"},{"location":"reference/preconditioners/#GaussianMarkovRandomFields.AbstractPreconditioner","page":"Preconditioners","title":"GaussianMarkovRandomFields.AbstractPreconditioner","text":"AbstractPreconditioner\n\nAbstract type for preconditioners. Should implement the following methods:\n\nldiv!(y, P::AbstractPreconditioner, x::AbstractVector)\nldiv!(P::AbstractPreconditioner, x::AbstractVector)\n\\(P::AbstractPreconditioner, x::AbstractVector)\nsize(P::AbstractPreconditioner)\n\n\n\n\n\n","category":"type"},{"location":"reference/preconditioners/#GaussianMarkovRandomFields.BlockJacobiPreconditioner","page":"Preconditioners","title":"GaussianMarkovRandomFields.BlockJacobiPreconditioner","text":"BlockJacobiPreconditioner\n\nA preconditioner that uses a block Jacobi preconditioner, i.e. P = diag(A₁, A₂, ...), where each Aᵢ is a preconditioner for a block of the matrix.\n\n\n\n\n\n","category":"type"},{"location":"reference/preconditioners/#GaussianMarkovRandomFields.FullCholeskyPreconditioner","page":"Preconditioners","title":"GaussianMarkovRandomFields.FullCholeskyPreconditioner","text":"FullCholeskyPreconditioner\n\nA preconditioner that uses a full Cholesky factorization of the matrix, i.e. P = A, so P⁻¹ = A⁻¹. Does not make sense to use on its own, but can be used as a building block for more complex preconditioners.\n\n\n\n\n\n","category":"type"},{"location":"reference/preconditioners/#GaussianMarkovRandomFields.temporal_block_gauss_seidel","page":"Preconditioners","title":"GaussianMarkovRandomFields.temporal_block_gauss_seidel","text":"temporal_block_gauss_seidel(A, block_size)\n\nConstruct a temporal block Gauss-Seidel preconditioner for a spatiotemporal matrix with constant spatial mesh size (and thus constant spatial block size).\n\n\n\n\n\n","category":"function"},{"location":"reference/preconditioners/#GaussianMarkovRandomFields.TridiagonalBlockGaussSeidelPreconditioner","page":"Preconditioners","title":"GaussianMarkovRandomFields.TridiagonalBlockGaussSeidelPreconditioner","text":"TridiagonalBlockGaussSeidelPreconditioner{T}(D_blocks, L_blocks)\nTridiagonalBlockGaussSeidelPreconditioner{T}(D⁻¹_blocks, L_blocks)\n\nBlock Gauss-Seidel preconditioner for block tridiagonal matrices. For a matrix given by\n\nA = beginbmatrix\nD₁  L₁ᵀ  0  cdots  0 \nL₁  D₂  L₂ᵀ  0  cdots \n0  L₂  D₃  L₃ᵀ  cdots \nvdots  vdots  vdots  ddots  vdots \n0  cdots  0  Lₙ₁  Lₙ\nendbmatrix\n\nthis preconditioner is given by\n\nP = beginbmatrix\nD₁  0  cdots  0 \nL₁  D₂  0  cdots \n0  L₂  D₃  cdots \nvdots  vdots  vdots  ddots  vdots \n0  cdots  0  Lₙ₁  Dₙ\nendbmatrix\n\nSolving linear systems with the preconditioner is made efficient through block forward / backward substitution. The diagonal blocks must be inverted. As such, they may be specified\n\ndirectly as matrices: in this case they will be transformed into FullCholeskyPreconditioners.\nin terms of their invertible preconditioners\n\n\n\n\n\n","category":"type"},{"location":"reference/preconditioners/#GaussianMarkovRandomFields.TridiagSymmetricBlockGaussSeidelPreconditioner","page":"Preconditioners","title":"GaussianMarkovRandomFields.TridiagSymmetricBlockGaussSeidelPreconditioner","text":"TridiagSymmetricBlockGaussSeidelPreconditioner{T}(D_blocks, L_blocks)\nTridiagSymmetricBlockGaussSeidelPreconditioner{T}(D⁻¹_blocks, L_blocks)\n\nSymmetric Block Gauss-Seidel preconditioner for symmetric block tridiagonal matrices. For a symmetric matrix given by the block decomposition A = L + D + Lᵀ, where L is strictly lower triangular and D is diagonal, this preconditioner is given by P = (L + D) D⁻¹ (L + D)ᵀ ≈ A.\n\nSolving linear systems with the preconditioner is made efficient through block forward / backward substitution. The diagonal blocks must be inverted. As such, they may be specified\n\ndirectly as matrices: in this case they will be transformed into FullCholeskyPreconditioners.\nin terms of their invertible preconditioners\n\n\n\n\n\n","category":"type"},{"location":"reference/plotting/#Plotting","page":"Plotting","title":"Plotting","text":"","category":"section"},{"location":"reference/plotting/","page":"Plotting","title":"Plotting","text":"GaussianMarkovRandomFields.jl offers recipes to make plotting spatial and spatiotemporal GMRFs effortless. These recipes are contained in a package extension which gets loaded automatically when using Makie (or rather: one of its backends).","category":"page"},{"location":"reference/plotting/","page":"Plotting","title":"Plotting","text":"All of the following methods may also be called more simply by calling plot or plot! with appropriate arguments.","category":"page"},{"location":"reference/plotting/","page":"Plotting","title":"Plotting","text":"note: Note\nCurrently, we only provide recipes for 1D spatial and spatiotemporal GMRFs. This may change soon (feel free to open a PR!) Until then, note that any 2D or 3D FEM-based GMRF may be plotted indirectly through Ferrite's support of VTK files, which may then subsequently be opened e.g. in ParaView for visualization. See Spatial Modelling with SPDEs for an example.","category":"page"},{"location":"reference/plotting/#GaussianMarkovRandomFields.gmrf_fem_1d_plot","page":"Plotting","title":"GaussianMarkovRandomFields.gmrf_fem_1d_plot","text":"gmrf_fem_1d_plot(gmrf::AbstractGMRF, disc::FEMDiscretization{1})\n\nPlot the GMRF gmrf based on the 1D FEM discretization disc.\n\nArguments\n\ngmrf::AbstractGMRF: The GMRF to plot.\ndisc::FEMDiscretization{1}: The FEM discretization.\n\nKeyword arguments\n\nwith_std::Bool=true: Whether to plot the confidence bands.\nN_samples::Int=3: The number of samples to plot.\nrng::AbstractRNG=Random.default_rng(): The random number generator.\nfield::Symbol=:default: The field to plot.\nmean_color: The color of the mean line.\nconf_band_color: The color of the confidence bands.\nsample_color: The color of the samples.\n\n\n\n\n\n","category":"function"},{"location":"reference/plotting/#GaussianMarkovRandomFields.gmrf_fem_1d_spatiotemporal_plot","page":"Plotting","title":"GaussianMarkovRandomFields.gmrf_fem_1d_spatiotemporal_plot","text":"gmrf_fem_1d_spatiotemporal_plot(\n    gmrf::Union{\n        ConstantMeshSTGMRF{1}, LinearConditionalGMRF{<:ConstantMeshSTGMRF{1}}\n        },\n    t_idx::Int\n)\n\nPlot the 1D spatiotemporal GMRF gmrf at time index t_idx.\n\nArguments\n\ngmrf: The GMRF to plot.\nt_idx::Int: The time index.\n\nKeyword arguments\n\nwith_std::Bool=true: Whether to plot the confidence bands.\nN_samples::Int=3: The number of samples to plot.\nrng::AbstractRNG=Random.default_rng(): The random number generator.\nfield::Symbol=:default: The field to plot.\nmean_color: The color of the mean line.\nconf_band_color: The color of the confidence bands.\nsample_color: The color of the samples.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/autoregressive_models/#Building-autoregressive-models","page":"Building autoregressive models","title":"Building autoregressive models","text":"","category":"section"},{"location":"tutorials/autoregressive_models/#Introduction","page":"Building autoregressive models","title":"Introduction","text":"","category":"section"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"In the following, we will construct a first-order auto-regressive model with Gaussian errors. Mathematically, this is expressed by","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"x_0 sim mathcalN(mu_0 Lambda_0) \nx_t+1 = phi x_t + varepsilon_t quad varepsilon sim mathcalN(0 Lambda)","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"The latter equation is equivalent to the likelihood","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"x_t+1 mid x_t sim mathcalN(phi x_t Lambda)","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Under this model, the joint distribution over x_1 dots x_N (where N is the number of steps) is Gaussian:","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"beginpmatrix x_0  x_1  x_2  vdots  x_N-1  x_N endpmatrix\nsim mathcalNleft(\nbeginpmatrix mu_0  phi mu_0  phi^2 mu_0  vdots \nphi^N-1 mu_0  phi^N mu_0 endpmatrix\nbeginpmatrix Lambda_0  -phi      \n-phi  Lambda + phi^2  -phi     \n      \n  ddots  ddots  ddots   \n      \n    -phi  Lambda + phi^2  -phi \n     -phi  Lambda\nendpmatrix^-1\nright)","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"The first-order Markov structure of the model results in a tridiagonal precision matrix. Thus, if we work with this Gaussian distribution in precision form (also commonly called information form), we gain tremendous computational benefits. By contrast, the covariance matrix of this Gaussian is fully dense.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"More generally, this package deals with any sparse precision matrix, not just tridiagonal ones. Such Gaussians with sparse precision matrices are called GMRFs (short for Gaussian Markov Random Fields).","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"In the following, we construct a GMRF for the above first-order autoregressive model first manually by computing the mean and precision, and then automatically by simply specifying the parameters of the model.","category":"page"},{"location":"tutorials/autoregressive_models/#Building-an-AR(1)-model","page":"Building autoregressive models","title":"Building an AR(1) model","text":"","category":"section"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"We begin by loading GaussianMarkovRandomFields and LinearAlgebra.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"using GaussianMarkovRandomFields, LinearAlgebra, SparseArrays","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"We define a discretization of the real interval 0 1, and specify some example parameters for the AR(1) model:","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"xs = 0:0.01:1\nN = length(xs)\nϕ = 0.995\nΛ₀ = 1.0e6\nΛ = 1.0e3","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Now we compute the mean and the precision matrix of the joint distribution. We explicitly declare the precision matrix as a symmetric tridiagonal matrix, which unlocks highly efficient linear algebra routines for the underlying computations.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"μ = [ϕ^(i - 1) for i in eachindex(xs)]\nmain_diag = [[Λ₀]; repeat([Λ + ϕ^2], N - 2); [Λ]]\noff_diag = repeat([-ϕ], N - 1)\nQ = sparse(SymTridiagonal(main_diag, off_diag))\nx = GMRF(μ, Q)","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"A GMRF is a multivariate Gaussian, and it's compatible with Distributions.jl. We can get its mean, marginal standard deviation, and draw samples as follows:","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"using Plots, Distributions\nplot(xs, mean(x), ribbon = 1.96 * std(x), label = \"Mean + std\")\nfor i in 1:3\n    plot!(xs, rand(x), fillalpha = 0.3, linestyle = :dash, label = \"Sample\")\nend\nplot!()","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Great! Looks like an AR(1) model.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"But what can you do with this? Well, for example you can use it as a prior for Bayesian inference. If we have a likelihood of the form","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"y mid x sim mathcalN(Ax Lambda_textobs^-1)","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"then the posterior conditioned on these observations is again a GMRF, the moments of which we can compute in closed form. In terms of code, we achieve this through condition_on_observations. Let's condition our model on the observations x_26 = 085 and x_76 = 071:","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"using SparseArrays\nA = spzeros(2, N)\nA[1, 26] = 1.0\nA[2, 76] = 1.0\ny = [0.85, 0.71]\nΛ_obs = 1.0e6\nx_cond = condition_on_observations(x, A, Λ_obs, y)","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Indeed, our model now conforms to these observations:","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"plot(xs, mean(x_cond), ribbon = 1.96 * std(x_cond), label = \"Mean + std\")\nfor i in 1:3\n    plot!(xs, rand(x_cond), fillalpha = 0.3, linestyle = :dash, label = \"Sample\")\nend\nplot!()","category":"page"},{"location":"tutorials/autoregressive_models/#Beyond-first-order-models:-CARs","page":"Building autoregressive models","title":"Beyond first-order models: CARs","text":"","category":"section"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"You may have noticed that the AR(1) model above produces very rough samples. This may or may not be desirable, depending on the application. If we do want smoother samples, we can increase the order of the model. This adds off-diagonals to the precision matrix and thus reduces its sparsity, so computations become a bit more expensive. But it may be worth the overhead.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"One model class to produce autoregressive models with flexible conditional dependencies and sparse precision matrices is that of conditional autoregressive models (CARs). Such models are constructed based on a graph representation of the underlying data, where there is an edge between two nodes if they are conditionally dependent.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Let us construct an adjacency matrix that relates nodes not only to their immediate neighbors, but also to the neighbors' neighbors (a second-order model).","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"W = spzeros(N, N)\nfor i in 1:N\n    for k in [-2, -1, 1, 2]\n        j = i + k\n        if 1 <= j <= N\n            W[i, j] = 1.0 / abs(k)\n        end\n    end\nend","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Now that we have the adjacency matrix, we can use a GaussianMarkovRandomFields.jl utility function to generate a CAR model from it, which internally constructs a slight variation of the graph Laplacian to form the precision matrix.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"x_car = generate_car_model(W, 0.9; μ = μ, σ = 0.001)","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Let's take our CAR for a test drive:","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"plot(xs, mean(x_car), ribbon = 1.96 * std(x_car), label = \"Mean + std\")\nfor i in 1:3\n    plot!(xs, rand(x_car), fillalpha = 0.3, linestyle = :dash, label = \"Sample\")\nend\nplot!()","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"Let's see how this model fits data. We take the same observations as for the AR(1) model, but also add an observation for the starting point to reduce the uncertainty there.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"A = spzeros(3, N)\nA[1, 1] = 1.0\nA[2, 26] = 1.0\nA[3, 76] = 1.0\ny = [1.0, 0.85, 0.71]\nΛ_obs = 1.0e6\nx_car_cond = condition_on_observations(x_car, A, Λ_obs, y)\nplot(xs, mean(x_car_cond), ribbon = 1.96 * std(x_car_cond), label = \"Mean + std\")\nfor i in 1:3\n    plot!(xs, rand(x_car_cond), fillalpha = 0.3, linestyle = :dash, label = \"Sample\")\nend\nplot!()","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"As expected, the interpolation of this model is less abrupt and spiky than for the AR(1) model.","category":"page"},{"location":"tutorials/autoregressive_models/#Outlook","page":"Building autoregressive models","title":"Outlook","text":"","category":"section"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"CAR models are quite flexible. Particularly for spatial data however, it is more common to model continuously through a Gaussian process. Fortunately, it turns out that popular Gaussian processes can be approximated quite nicely through GMRFs, allowing us to do the modelling in terms of a GP and the computations in terms of a GMRF. To learn more about this approach, check the tutorial on Spatial Modelling with SPDEs.","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"","category":"page"},{"location":"tutorials/autoregressive_models/","page":"Building autoregressive models","title":"Building autoregressive models","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/autodiff/#Automatic-Differentiation","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"GaussianMarkovRandomFields.jl provides automatic differentiation (AD) support for parameter estimation, Bayesian inference, and optimization workflows involving GMRFs. Two complementary approaches are available, each with distinct strengths and use cases.","category":"page"},{"location":"reference/autodiff/#Zygote-with-custom-chainrules","page":"Automatic Differentiation","title":"Zygote with custom chainrules","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The primary AD implementation uses ChainRulesCore.jl to provide efficient reverse-mode automatic differentiation rules. Zygote.jl will automatically load and use these rules. The implementation leverages SelectedInversion.jl to compute gradients efficiently without materializing full covariance matrices. ","category":"page"},{"location":"reference/autodiff/#Supported-Operations","page":"Automatic Differentiation","title":"Supported Operations","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"GMRF Construction: Differentiation through GMRF(μ, Q, solver_blueprint) and GMRF(μ, Q) constructors, enabling gradients to flow back to mean vectors and precision matrices.","category":"page"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Log-probability Density: Efficient differentiation of logpdf(gmrf, z) computations using selected inverses.","category":"page"},{"location":"reference/autodiff/#Current-Limitations","page":"Automatic Differentiation","title":"Current Limitations","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Conditional GMRFs: Chain rules do not currently support condition_on_observations. For workflows requiring conditional inference with AD, use the LDLFactorizations approach below. Contributions to extend chain rules support to conditional operations are welcome.","category":"page"},{"location":"reference/autodiff/#Solver-Compatibility","page":"Automatic Differentiation","title":"Solver Compatibility","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Chain rules work with any Cholesky-based solver backend:","category":"page"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"CHOLMOD (default): Fast sparse Cholesky via SuiteSparse\nPardiso: Pardiso solver (via package extension)\nLDLFactorizations: Pure Julia implementation of a Cholesky solver","category":"page"},{"location":"reference/autodiff/#Basic-Usage-Example","page":"Automatic Differentiation","title":"Basic Usage Example","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"using GaussianMarkovRandomFields, Zygote, SparseArrays\n\n# Define precision matrix\nQ = spdiagm(-1 => -0.5*ones(99), 0 => ones(100), 1 => -0.5*ones(99))\n\n# Sample point for evaluation\nz = randn(100)\n\n# Define function to differentiate\ngmrf_from_mean = θ -> GMRF(θ, Q)\nlogpdf_from_mean = θ -> logpdf(gmrf_from_mean(θ), z)\n\n# Differentiate\nθ_eval = zeros(100)\ngradient(logpdf_from_mean, θ_eval)[1]","category":"page"},{"location":"reference/autodiff/#LDLFactorizations-Approach","page":"Automatic Differentiation","title":"LDLFactorizations Approach","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The alternative approach uses LDLFactorizations.jl, a plain Julia implementation of sparse Cholesky factorization that supports automatic differentiation through all Julia AD libraries. This \"just works\", but you're limited to LDLFactorizations.jl. This may be fine for moderately sized problems, but CHOLMOD and Pardiso will generally scale much more efficiently.","category":"page"},{"location":"reference/autodiff/#When-to-Use","page":"Automatic Differentiation","title":"When to Use","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Conditional GMRFs: (Currently) required for condition_on_observations with AD\nForward-mode AD: Efficient for problems with few parameters","category":"page"},{"location":"reference/autodiff/#Solver-Configuration","page":"Automatic Differentiation","title":"Solver Configuration","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Use the :autodiffable solver variant:","category":"page"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"# Configure autodiffable solver\nblueprint = CholeskySolverBlueprint{:autodiffable}()\ngmrf = GMRF(μ, Q, blueprint)","category":"page"},{"location":"reference/autodiff/#Troubleshooting","page":"Automatic Differentiation","title":"Troubleshooting","text":"","category":"section"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Error: \"Automatic differentiation through logpdf currently only supports Cholesky-based solvers\"","category":"page"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Solution: Ensure your GMRF uses a Cholesky-based solver, not CGSolver or other iterative methods","category":"page"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Poor performance with chain rules","category":"page"},{"location":"reference/autodiff/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Check if you're accidentally using :autodiffable solver when chain rules would work with default solver","category":"page"},{"location":"tutorials/#Tutorials","page":"Overview","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/","page":"Overview","title":"Overview","text":"Pages = [\n    \"autoregressive_models.md\",\n    \"spatial_modelling_spdes.md\",\n    \"boundary_conditions.md\"\n]","category":"page"},{"location":"reference/gaussian_approximation/#Gaussian-Approximation","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"","category":"section"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"When using non-Gaussian observation models (Poisson, Bernoulli, etc.) with GMRF priors, the posterior distribution p(x|y) is no longer Gaussian. Gaussian approximation finds the \"best\" Gaussian distribution that approximates this intractable posterior, enabling efficient inference using the full power of GMRF machinery.","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"The gaussian_approximation function constructs this approximation by finding the posterior mode and using the curvature at the mode to define a Gaussian with matching first and second moments. This unlocks fast sampling, marginal variance computation, and further conditioning operations.","category":"page"},{"location":"reference/gaussian_approximation/#Conjugate-vs-Non-Conjugate-Cases","page":"Gaussian Approximation","title":"Conjugate vs Non-Conjugate Cases","text":"","category":"section"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"The package automatically detects two fundamentally different cases:","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"Conjugate Case (Exact): Normal observations with identity link result in a Gaussian posterior that can be computed exactly using linear conditioning. No approximation is needed.","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"Non-Conjugate Case (Approximate): All other observation models require iterative optimization using Fisher scoring to find the mode and Hessian of the log-posterior.","category":"page"},{"location":"reference/gaussian_approximation/#Basic-Usage-Pattern","page":"Gaussian Approximation","title":"Basic Usage Pattern","text":"","category":"section"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"# Step 1: Set up prior GMRF\nprior_gmrf = GMRF(μ_prior, Q_prior)\n\n# Step 2: Set up observation model and materialize likelihood\nobs_model = ExponentialFamily(Poisson)  # or Normal, Bernoulli, etc.\nobs_lik = obs_model(y_data; hyperparameters...)\n\n# Step 3: Find Gaussian approximation to posterior\nposterior_gmrf = gaussian_approximation(prior_gmrf, obs_lik)\n\n# Step 4: Use like any other GMRF\nsample = rand(posterior_gmrf)\nposterior_mean = mean(posterior_gmrf)\nmarginal_stds = std(posterior_gmrf)","category":"page"},{"location":"reference/gaussian_approximation/#Examples","page":"Gaussian Approximation","title":"Examples","text":"","category":"section"},{"location":"reference/gaussian_approximation/#Conjugate-Case:-Normal-Observations","page":"Gaussian Approximation","title":"Conjugate Case: Normal Observations","text":"","category":"section"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"For Normal observations with identity link, the posterior is exactly Gaussian:","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"using GaussianMarkovRandomFields, Distributions, LinearAlgebra\n\n# Prior: zero mean, unit precision\nn = 10\nprior_gmrf = GMRF(zeros(n), Diagonal(ones(n)))\n\n# Normal observations: y ~ N(x, σ²I) \nobs_model = ExponentialFamily(Normal)\ny = randn(n)  # Some observed data\nobs_lik = obs_model(y; σ=0.5)\n\n# Exact posterior (no iteration needed!)\nposterior_gmrf = gaussian_approximation(prior_gmrf, obs_lik)","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"This is mathematically equivalent to Bayesian linear regression and is computed exactly using the conjugate prior relationship.","category":"page"},{"location":"reference/gaussian_approximation/#Non-Conjugate-Case:-Poisson-GLM","page":"Gaussian Approximation","title":"Non-Conjugate Case: Poisson GLM","text":"","category":"section"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"For count data with log-link, we get an approximate Gaussian posterior:","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"# Prior for log-intensities\nprior_gmrf = GMRF(zeros(n), Diagonal(ones(n)))\n\n# Poisson observations: y ~ Poisson(exp(x))\nobs_model = ExponentialFamily(Poisson)  # Uses LogLink by default\ny_counts = [3, 1, 4, 1, 5, 2, 6, 3, 5, 4]  # Count data\nobs_lik = obs_model(y_counts)\n\n# Approximate Gaussian posterior via Fisher scoring\nposterior_gmrf = gaussian_approximation(prior_gmrf, obs_lik)","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"The approximation quality depends on how \"Gaussian-like\" the true posterior is. For moderate counts and reasonable prior precision, the approximation is typically excellent.","category":"page"},{"location":"reference/gaussian_approximation/#Design-Matrices:-Still-Conjugate","page":"Gaussian Approximation","title":"Design Matrices: Still Conjugate","text":"","category":"section"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"Even with linear transformations, Normal observations remain conjugate:","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"# GLM-style design matrix: intercept + covariate\nX = [1.0 2.1; 1.0 3.4; 1.0 1.8; 1.0 4.2]  # 4 obs, 2 coefficients\nn_coef = 2\n\n# Prior on coefficients [β₀, β₁]\nprior_gmrf = GMRF(zeros(n_coef), Diagonal(ones(n_coef)))\n\n# Normal observations with design matrix\nbase_model = ExponentialFamily(Normal)\nobs_model = LinearlyTransformedObservationModel(base_model, X)\ny = [2.3, 3.8, 2.0, 4.5]  # Response data\nobs_lik = obs_model(y; σ=0.3)\n\n# Still exact! Uses linear conditioning internally\nposterior_gmrf = gaussian_approximation(prior_gmrf, obs_lik)","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"This covers standard GLM scenarios while maintaining computational efficiency.","category":"page"},{"location":"reference/gaussian_approximation/#Performance-Notes","page":"Gaussian Approximation","title":"Performance Notes","text":"","category":"section"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"The package includes significant performance optimizations:","category":"page"},{"location":"reference/gaussian_approximation/","page":"Gaussian Approximation","title":"Gaussian Approximation","text":"Conjugate cases are detected automatically and use exact linear conditioning instead of iterative optimization, providing both speed and numerical accuracy benefits\nNon-conjugate cases use efficient Fisher scoring with good initial guesses from the prior mean\nMetaGMRF support preserves metadata through the approximation process\nType consistency ensures optimal memory usage and dispatch efficiency","category":"page"},{"location":"reference/gaussian_approximation/#API-Reference","page":"Gaussian Approximation","title":"API Reference","text":"","category":"section"},{"location":"reference/gaussian_approximation/#GaussianMarkovRandomFields.gaussian_approximation","page":"Gaussian Approximation","title":"GaussianMarkovRandomFields.gaussian_approximation","text":"gaussian_approximation(prior_gmrf, obs_lik) -> AbstractGMRF\n\nFind Gaussian approximation to the posterior using Fisher scoring.\n\nThis function finds the mode of the posterior distribution and constructs a Gaussian approximation around it using Fisher scoring (Newton-Raphson with Fisher information matrix).\n\nArguments\n\nprior_gmrf: Prior GMRF distribution for the latent field\nobs_lik: Materialized observation likelihood (contains data and hyperparameters)\n\nReturns\n\nposterior_gmrf::GMRF: Gaussian approximation to the posterior p(x | θ, y)\n\nExample\n\n# Set up components (done at higher level)\nprior_gmrf = GMRF(μ_prior, Q_prior)\nobs_model = ExponentialFamily(Poisson)\nobs_lik = obs_model(y)  # Materialized once\n\n# Find Gaussian approximation - returns a GMRF\nposterior_gmrf = gaussian_approximation(prior_gmrf, obs_lik)\n\n\n\n\n\ngaussian_approximation(prior_constrained::ConstrainedGMRF, obs_lik::ObservationLikelihood; kwargs...)\n\nFind Gaussian approximation to the constrained posterior using Newton optimization with constraint projection.\n\nAlternates between Newton/Fisher scoring steps and projecting onto the constraint manifold, which is mathematically equivalent to using Schur complements in the KKT approach for constrained Newton optimization.\n\nArguments\n\nprior_constrained::ConstrainedGMRF: Prior constrained GMRF distribution  \nobs_lik::ObservationLikelihood: Materialized observation likelihood\n\nKeyword Arguments\n\nmax_iter::Int=50: Maximum number of Newton iterations\nmean_change_tol::Real=1e-4: Convergence tolerance for mean change\nnewton_dec_tol::Real=1e-5: Newton decrement convergence tolerance  \nverbose::Bool=false: Print iteration information\n\nReturns\n\nposterior_constrained::ConstrainedGMRF: Constrained Gaussian approximation to posterior\n\n\n\n\n\n","category":"function"},{"location":"reference/linear_maps/#Linear-Maps","page":"Linear maps","title":"Linear Maps","text":"","category":"section"},{"location":"reference/linear_maps/","page":"Linear maps","title":"Linear maps","text":"The construction of GMRFs involves various kinds of structured matrices. These structures may be leveraged in downstream computations to save compute and memory. But to make this possible, we need to actually keep track of these structures -  which we achieve through diverse subtypes of LinearMap.","category":"page"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.SymmetricBlockTridiagonalMap","page":"Linear maps","title":"GaussianMarkovRandomFields.SymmetricBlockTridiagonalMap","text":"SymmetricBlockTridiagonalMap(\n    diagonal_blocks::Tuple{LinearMap{T},Vararg{LinearMap{T},ND}},\n    off_diagonal_blocks::Tuple{LinearMap{T},Vararg{LinearMap{T},NOD}},\n)\n\nA linear map representing a symmetric block tridiagonal matrix with diagonal blocks diagonal_blocks and lower off-diagonal blocks off_diagonal_blocks.\n\n\n\n\n\n","category":"type"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.SSMBidiagonalMap","page":"Linear maps","title":"GaussianMarkovRandomFields.SSMBidiagonalMap","text":"SSMBidiagonalMap{T}(\n    A::LinearMap{T},\n    B::LinearMap{T},\n    C::LinearMap{T},\n    N_t::Int,\n)\n\nRepresents the block-bidiagonal map given by the (Nt) x (Nt - 1) sized block structure:\n\nbeginbmatrix\nA  0  cdots  0 \nB  C  cdots  0 \n0  B  C  0 \nvdots  vdots  ddots  vdots \n0  0  cdots  B\nendbmatrix\n\nwhich occurs as a square root in the discretization of GMRF-based state-space models. N_t is the total number of blocks along the rows.\n\n\n\n\n\n","category":"type"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.OuterProductMap","page":"Linear maps","title":"GaussianMarkovRandomFields.OuterProductMap","text":"OuterProductMap{T}(\n    A::LinearMap{T},\n    Q::LinearMap{T},\n)\n\nRepresents the outer product A' Q A, without actually forming it in memory.\n\n\n\n\n\n","category":"type"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.LinearMapWithSqrt","page":"Linear maps","title":"GaussianMarkovRandomFields.LinearMapWithSqrt","text":"LinearMapWithSqrt{T}(\n    A::LinearMap{T},\n    A_sqrt::LinearMap{T},\n)\n\nA symmetric positive definite linear map A with known square root A_sqrt, i.e. A = A_sqrt * A_sqrt'. Behaves just like A, but taking the square root directly returns A_sqrt.\n\nArguments\n\nA::LinearMap{T}: The linear map A.\nA_sqrt::LinearMap{T}: The square root of A.\n\n\n\n\n\n","category":"type"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.CholeskySqrt","page":"Linear maps","title":"GaussianMarkovRandomFields.CholeskySqrt","text":"CholeskySqrt(cho::Union{Cholesky{T},SparseArrays.CHOLMOD.Factor{T}})\n\nA linear map representing the square root obtained from a Cholesky factorization, i.e. for A = L * L', this map represents L.\n\nArguments\n\ncho::Union{Cholesky{T},SparseArrays.CHOLMOD.Factor{T}}:   The Cholesky factorization of a symmetric positive definite matrix.\n\n\n\n\n\n","category":"function"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.CholeskyFactorizedMap","page":"Linear maps","title":"GaussianMarkovRandomFields.CholeskyFactorizedMap","text":"CholeskyFactorizedMap{T,C}(cho::C) where {T,C}\n\nA linear map represented in terms of its Cholesky factorization, i.e. for A = L * L', this map represents A.\n\nType Parameters\n\nT: Element type of the matrix\nC: Type of the Cholesky factorization\n\nArguments\n\ncho: The Cholesky factorization of a symmetric positive definite matrix. Can be Cholesky, SparseArrays.CHOLMOD.Factor, or LDLFactorization.\n\n\n\n\n\n","category":"type"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.ZeroMap","page":"Linear maps","title":"GaussianMarkovRandomFields.ZeroMap","text":"ZeroMap{T}(N::Int, M::Int)\n\nA linear map that maps all vectors to the zero vector.\n\nArguments\n\nN::Int: Output dimension\nM::Int: Input dimension\n\n\n\n\n\n","category":"type"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.ADJacobianMap","page":"Linear maps","title":"GaussianMarkovRandomFields.ADJacobianMap","text":"ADJacobianMap(f::Function, x₀::AbstractVector{T}, N_outputs::Int)\n\nA linear map representing the Jacobian of f at x₀. Uses forward-mode AD in a matrix-free way, i.e. we do not actually store the Jacobian in memory and only compute JVPs.\n\nRequires ForwardDiff.jl!\n\nArguments\n\nf::Function: Function to differentiate.\nx₀::AbstractVector{T}: Input vector at which to evaluate the Jacobian.\nN_outputs::Int: Output dimension of f.\n\n\n\n\n\n","category":"type"},{"location":"reference/linear_maps/#GaussianMarkovRandomFields.ADJacobianAdjointMap","page":"Linear maps","title":"GaussianMarkovRandomFields.ADJacobianAdjointMap","text":"ADJacobianAdjointMap{T}(f::Function, x₀::AbstractVector{T}, N_outputs::Int)\n\nA linear map representing the adjoint of the Jacobian of f at x₀. Uses reverse-mode AD in a matrix-free way, i.e. we do not actually store the Jacobian in memory and only compute VJPs.\n\nRequires Zygote.jl!\n\nArguments\n\nf::Function: Function to differentiate.\nx₀::AbstractVector{T}: Input vector at which to evaluate the Jacobian.\nN_outputs::Int: Output dimension of f.\n\n\n\n\n\n","category":"type"},{"location":"dev-docs/spdes/#SPDEs","page":"SPDEs","title":"SPDEs","text":"","category":"section"},{"location":"dev-docs/spdes/#GaussianMarkovRandomFields.matern_mean_precision","page":"SPDEs","title":"GaussianMarkovRandomFields.matern_mean_precision","text":"matern_precision(C_inv::AbstractMatrix, K::AbstractMatrix, α::Integer)\n\nCompute the precision matrix of a GMRF discretization of a Matérn SPDE. Implements the recursion described in [1].\n\nArguments\n\nC_inv::AbstractMatrix: The inverse of the (possibly lumped) mass matrix.\nK::AbstractMatrix: The stiffness matrix.\nα::Integer: The parameter α = ν + d/2 of the Matérn SPDE.\n\n\n\n\n\n","category":"function"},{"location":"reference/#API-Reference-overview","page":"Overview","title":"API Reference overview","text":"","category":"section"},{"location":"reference/","page":"Overview","title":"Overview","text":"Pages = [\n    \"gmrfs.md\",\n    \"observation_models.md\",\n    \"gaussian_approximation.md\",\n    \"hard_constraints.md\",\n    \"spdes.md\",\n    \"discretizations.md\",\n    \"meshes.md\",\n    \"plotting.md\",\n    \"solvers.md\",\n    \"autoregressive.md\",\n    \"linear_maps.md\",\n    \"preconditioners.md\",\n    \"autodiff.md\",\n]","category":"page"},{"location":"reference/autoregressive/#Autoregressive-Models","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"section"},{"location":"reference/autoregressive/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For a hands-on example, check out the tutorial Building autoregressive models.","category":"page"},{"location":"reference/autoregressive/#GaussianMarkovRandomFields.generate_car_model","page":"Autoregressive Models","title":"GaussianMarkovRandomFields.generate_car_model","text":"generate_car_model(W::SparseMatrixCSC, ρ::Real; σ=1.0, μ=nothing)\n\nGenerate a conditional autoregressive model (CAR) in GMRF form from an adjacency matrix.\n\nInput\n\nW – Adjacency / weight matrix. Specifies the conditional dependencies        between variables\nρ – Weighting factor of the inter-node dependencies. Fulfills 0 < ρ < 1.\nσ – Variance scaling factor (i.e. output scale)\nμ – Mean vector\n\nOutput\n\nA GMRF with the corresponding mean and precision.\n\nAlgorithm\n\nThe CAR is constructed using a variant of the graph Laplacian, i.e.\n\nQ = sigma^-1 cdot (W 1 - ρ cdot W)\n\n\n\n\n\n","category":"function"},{"location":"reference/spdes/#SPDEs","page":"SPDEs","title":"SPDEs","text":"","category":"section"},{"location":"reference/spdes/#GaussianMarkovRandomFields.SPDE","page":"SPDEs","title":"GaussianMarkovRandomFields.SPDE","text":"SPDE\n\nAn abstract type for a stochastic partial differential equation (SPDE).\n\n\n\n\n\n","category":"type"},{"location":"reference/spdes/#GaussianMarkovRandomFields.MaternSPDE","page":"SPDEs","title":"GaussianMarkovRandomFields.MaternSPDE","text":"MaternSPDE{D}(κ::Real, ν::Union{Integer, Rational}) where D\n\nThe Whittle-Matérn SPDE is given by\n\n(κ^2 - Δ)^fracα2 u(x) = 𝒲(x) quad left( x in mathbbR^d\nα = ν + fracd2 right)\n\nwhere Δ is the Laplacian operator, κ  0, ν  0.\n\nThe stationary solutions to this SPDE are Matérn processes.\n\n\n\n\n\n","category":"type"},{"location":"reference/spdes/#GaussianMarkovRandomFields.AdvectionDiffusionSPDE","page":"SPDEs","title":"GaussianMarkovRandomFields.AdvectionDiffusionSPDE","text":"AdvectionDiffusionSPDE{D}(κ::Real, α::Rational, H::AbstractMatrix,\nγ::AbstractVector, c::Real, τ::Real) where {D}\n\nSpatiotemporal advection-diffusion SPDE as proposed in [2]:\n\nleft fract + frac1c left( κ^2 -   H  right)^alpha\n+ frac1c γ   right X(t s) = fracτsqrtc Z(t s)\n\nwhere Z(t, s) is spatiotemporal noise which may be colored.\n\n\n\n\n\n","category":"type"},{"location":"reference/solvers/#Solvers","page":"Solvers","title":"Solvers","text":"","category":"section"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"Fundamentally, all interesting quantities of GMRFs (samples, marginal variances, posterior means, ...) must be computed through sparse linear algebra. GaussianMarkovRandomFields.jl uses LinearSolve.jl as the backend for all linear algebra operations, providing access to a wide range of modern solvers and preconditioners while maintaining a unified interface. Our goal is to provide sane defaults that \"just work\" for most users, while still allowing power users to customize the behavior through LinearSolve.jl algorithms. Read further to learn about the available capabilities and how to use them.","category":"page"},{"location":"reference/solvers/#Direct-Methods-(Cholesky,-LU)","page":"Solvers","title":"Direct Methods (Cholesky, LU)","text":"","category":"section"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"CHOLMOD and other direct solvers are  state-of-the-art methods for sparse linear systems. LinearSolve.jl automatically selects appropriate direct methods (typically Cholesky for symmetric positive definite systems like GMRF precision matrices) to compute factorizations of the precision matrix, which are then leveraged to draw samples and compute posterior means. Marginal variances are computed using selected inversion when available, or RBMC as a fallback.","category":"page"},{"location":"reference/solvers/#Pardiso","page":"Solvers","title":"Pardiso","text":"","category":"section"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"Pardiso is a state-of-the-art direct solver for sparse linear systems with excellent performance on large-dimensional GMRFs and highly efficient methods for marginal variance computation.","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"To use Pardiso, you need to set up and load Pardiso.jl, then specify alg=LinearSolve.PardisoJL() when creating your GMRF:","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"using Pardiso, LinearSolve\ngmrf = GMRF(μ, Q, alg=LinearSolve.PardisoJL())","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"See the LinearSolve.jl documentation for more information about PardisoJL algorithm options.","category":"page"},{"location":"reference/solvers/#Iterative-Methods-(CG,-GMRES)","page":"Solvers","title":"Iterative Methods (CG, GMRES)","text":"","category":"section"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"The Conjugate Gradient (CG) method and other iterative methods are efficient approaches for solving large sparse symmetric positive-definite linear systems. LinearSolve.jl provides access to various iterative solvers that can be used for large-scale GMRFs where direct methods become prohibitively expensive.","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"For symmetric systems like GMRFs, CG-based methods are typically most appropriate:","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"using LinearSolve\ngmrf = GMRF(μ, Q, alg=LinearSolve.KrylovJL_CG())","category":"page"},{"location":"reference/solvers/#Variance-Computation-Strategies","page":"Solvers","title":"Variance Computation Strategies","text":"","category":"section"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"Fundamentally, computing marginal variances of GMRFs is not trivial, as it requires computing the diagonal entries of the covariance matrix (which is the inverse of the precision matrix).","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"Selected Inversion (including Takahashi recursions) is a highly accurate and stable method for computing these variances when available for the chosen algorithm. This is automatically used when supported.","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"When selected inversion is not available, the package automatically falls back to RBMC (Rao-Blackwellized Monte Carlo), a sampling-based approach that is less accurate but can be much faster for large GMRFs.","category":"page"},{"location":"reference/solvers/#GaussianMarkovRandomFields.AbstractVarianceStrategy","page":"Solvers","title":"GaussianMarkovRandomFields.AbstractVarianceStrategy","text":"AbstractVarianceStrategy\n\nAn abstract type for a strategy to compute the variance of a GMRF.\n\n\n\n\n\n","category":"type"},{"location":"reference/solvers/#GaussianMarkovRandomFields.RBMCStrategy","page":"Solvers","title":"GaussianMarkovRandomFields.RBMCStrategy","text":"RBMCStrategy(n_samples; rng)\n\nRao-Blackwellized Monte Carlo estimator of a GMRF's marginal variances based on [3]. Particularly useful in large-scale regimes where Takahashi recursions may be too expensive.\n\nArguments\n\nn_samples::Int: Number of samples to draw.\n\nKeyword arguments\n\nrng::Random.AbstractRNG = Random.default_rng(): Random number generator.\n\n\n\n\n\n","category":"type"},{"location":"reference/solvers/#GaussianMarkovRandomFields.BlockRBMCStrategy","page":"Solvers","title":"GaussianMarkovRandomFields.BlockRBMCStrategy","text":"BlockRBMCStrategy(n_samples; rng, enclosure_size)\n\nBlock Rao-Blackwellized Monte Carlo estimator of a GMRF's marginal variances based on [3]. Achieves faster convergence than plain RBMC by considering blocks of nodes rather than individual nodes, thus integrating more information about the precision matrix. enclosure_size specifies the size of these blocks. Larger values lead to faster convergence (in terms of the number of samples) at the cost of increased compute. Thus, one should aim for a sweet spot between sampling costs and block operation costs.\n\nArguments\n\nn_samples::Int: Number of samples to draw.\n\nKeyword arguments\n\nrng::Random.AbstractRNG = Random.default_rng(): Random number generator.\nenclosure_size::Int = 1: Size of the blocks.\n\n\n\n\n\n","category":"type"},{"location":"reference/solvers/#Advanced-Operations","page":"Solvers","title":"Advanced Operations","text":"","category":"section"},{"location":"reference/solvers/#GaussianMarkovRandomFields.logdet_cov","page":"Solvers","title":"GaussianMarkovRandomFields.logdet_cov","text":"logdet_cov(linsolve, alg)\n\nCompute the log determinant of the covariance matrix (inverse of precision). Dispatches on the algorithm type.\n\n\n\n\n\nlogdet_cov(linsolve)\n\nConvenience function that dispatches to logdet_cov(linsolve, linsolve.alg).\n\n\n\n\n\n","category":"function"},{"location":"reference/solvers/#GaussianMarkovRandomFields.selinv","page":"Solvers","title":"GaussianMarkovRandomFields.selinv","text":"selinv(linsolve, alg)\n\nCompute the full selected inverse matrix using selected inversion. Dispatches on the algorithm type.\n\n\n\n\n\nselinv(linsolve)\n\nConvenience function that dispatches to selinv(linsolve, linsolve.alg).\n\n\n\n\n\n","category":"function"},{"location":"reference/solvers/#GaussianMarkovRandomFields.backward_solve","page":"Solvers","title":"GaussianMarkovRandomFields.backward_solve","text":"backward_solve(linsolve, x, alg)\n\nPerform backward solve L^T \\ x where L is the Cholesky factor. Dispatches on the algorithm type.\n\n\n\n\n\nbackward_solve(linsolve, x)\n\nConvenience function that dispatches to backward_solve(linsolve, x, linsolve.alg).\n\n\n\n\n\n","category":"function"},{"location":"reference/solvers/#Choosing-a-Solver","page":"Solvers","title":"Choosing a Solver","text":"","category":"section"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"LinearSolve.jl automatically selects appropriate algorithms based on matrix properties:","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"Direct methods (Cholesky, LU) for small to medium-sized GMRFs\nIterative methods (CG, GMRES) for large sparse GMRFs where direct methods become prohibitively expensive","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"This matches our general recommendations: Direct solvers combined with selected inversion are highly accurate and stable and should be used whenever possible. However, direct solvers become prohibitively expensive for very large-scale GMRFs, both in terms of compute and memory use. In these regimes, iterative methods may still be a viable option.","category":"page"},{"location":"reference/solvers/","page":"Solvers","title":"Solvers","text":"If you have access to both strong parallel computing resources and a Pardiso license, LinearSolve.PardisoJL() may provide the best performance.","category":"page"},{"location":"dev-docs/discretizations/#Discretizations","page":"Discretizations","title":"Discretizations","text":"","category":"section"},{"location":"dev-docs/discretizations/#Spatial-discretization-with-FEM","page":"Discretizations","title":"Spatial discretization with FEM","text":"","category":"section"},{"location":"dev-docs/discretizations/#GaussianMarkovRandomFields.shape_gradient_local","page":"Discretizations","title":"GaussianMarkovRandomFields.shape_gradient_local","text":"shape_gradient_local(f::FEMDiscretization, shape_idx::Int, ξ)\n\nGradient of the shape function with index shape_idx with respect to the local coordinates ξ.\n\n\n\n\n\n","category":"function"},{"location":"dev-docs/discretizations/#GaussianMarkovRandomFields.shape_gradient_global","page":"Discretizations","title":"GaussianMarkovRandomFields.shape_gradient_global","text":"shape_gradient_global(f::FEMDiscretization, dof_coords, shape_idx::Int, ξ; J⁻¹ = nothing)\n\nGradient of the shape function with index shape_idx in a cell with node coordinates dof_coords, taken with respect to the global coordinates but computed in terms of the local coordinates ξ.\n\n\n\n\n\n","category":"function"},{"location":"dev-docs/discretizations/#GaussianMarkovRandomFields.shape_hessian_local","page":"Discretizations","title":"GaussianMarkovRandomFields.shape_hessian_local","text":"shape_hessian_local(f::FEMDiscretization, shape_idx::Int, ξ)\n\nHessian of the shape function with index shape_idx with respect to the local coordinates ξ.\n\n\n\n\n\n","category":"function"},{"location":"dev-docs/discretizations/#GaussianMarkovRandomFields.shape_hessian_global","page":"Discretizations","title":"GaussianMarkovRandomFields.shape_hessian_global","text":"shape_hessian_global(f::FEMDiscretization, dof_coords, shape_idx::Int, ξ; J⁻¹ = nothing, geo_hessian = nothing)\n\nHessian of the shape function with index shape_idx in a cell with node coordinates dof_coords, taken with respect to the global coordinates but computed in terms of the local coordinates ξ.\n\n\n\n\n\n","category":"function"},{"location":"dev-docs/discretizations/#GaussianMarkovRandomFields.geom_jacobian","page":"Discretizations","title":"GaussianMarkovRandomFields.geom_jacobian","text":"geom_jacobian(f::FEMDiscretization, dof_coords, ξ)\n\nJacobian of the geometry mapping at the local coordinates ξ with node coordinates dof_coords. By \"geometry mapping\", we mean the mapping from the reference element to the physical element.\n\n\n\n\n\n","category":"function"},{"location":"dev-docs/discretizations/#GaussianMarkovRandomFields.geom_hessian","page":"Discretizations","title":"GaussianMarkovRandomFields.geom_hessian","text":"geom_hessian(f::FEMDiscretization, dof_coords, ξ)\n\nHessian of the geometry mapping at the local coordinates ξ with node coordinates dof_coords. By \"geometry mapping\", we mean the mapping from the reference element to the physical element.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#Spatial-and-spatiotemporal-discretizations","page":"Discretizations","title":"Spatial and spatiotemporal discretizations","text":"","category":"section"},{"location":"reference/discretizations/#Discretizing-SPDEs","page":"Discretizations","title":"Discretizing SPDEs","text":"","category":"section"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.discretize","page":"Discretizations","title":"GaussianMarkovRandomFields.discretize","text":"discretize(𝒟::MaternSPDE{D}, discretization::FEMDiscretization{D})::AbstractGMRF where {D}\n\nDiscretize a Matérn SPDE using a Finite Element Method (FEM) discretization. Computes the stiffness and (lumped) mass matrix, and then forms the precision matrix of the GMRF discretization.\n\n\n\n\n\ndiscretize(spde::AdvectionDiffusionSPDE, discretization::FEMDiscretization,\nts::AbstractVector{Float64}; colored_noise = false,\nstreamline_diffusion = false, h = 0.1) where {D}\n\nDiscretize an advection-diffusion SPDE using a constant spatial mesh. Streamline diffusion is an optional stabilization scheme for advection-dominated problems, which are known to be unstable. When using streamline diffusion, h may be passed to specify the mesh element size.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#Spatial-discretization:-FEM","page":"Discretizations","title":"Spatial discretization: FEM","text":"","category":"section"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.FEMDiscretization","page":"Discretizations","title":"GaussianMarkovRandomFields.FEMDiscretization","text":"FEMDiscretization(\n    grid::Ferrite.Grid,\n    interpolation::Ferrite.Interpolation,\n    quadrature_rule::Ferrite.QuadratureRule,\n    fields = ((:u, nothing),),\n    boundary_conditions = (),\n)\n\nA struct that contains all the information needed to discretize an (S)PDE using the Finite Element Method.\n\nArguments\n\ngrid::Ferrite.Grid: The grid on which the discretization is defined.\ninterpolation::Ferrite.Interpolation: The interpolation scheme, i.e. the                                         type of FEM elements.\nquadrature_rule::Ferrite.QuadratureRule: The quadrature rule.\nfields::Vector{Tuple{Symbol, Union{Nothing, Ferrite.Interpolation}}}:       The fields to be discretized. Each tuple contains the field name and       the geometric interpolation scheme. If the interpolation scheme is       nothing, interpolation is used for geometric interpolation.\nboundary_conditions::Vector{Tuple{Ferrite.BoundaryCondition, Float64}}:       The (soft) boundary conditions. Each tuple contains the boundary       condition and the noise standard deviation.\n\n\n\n\n\n","category":"type"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.ndim","page":"Discretizations","title":"GaussianMarkovRandomFields.ndim","text":"ndim(f::FEMDiscretization)\n\nReturn the dimension of space in which the discretization is defined. Typically ndim(f) == 1, 2, or 3.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#Ferrite.ndofs-Tuple{FEMDiscretization}","page":"Discretizations","title":"Ferrite.ndofs","text":"ndofs(f::FEMDiscretization)\n\nReturn the number of degrees of freedom in the discretization.\n\n\n\n\n\n","category":"method"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.evaluation_matrix","page":"Discretizations","title":"GaussianMarkovRandomFields.evaluation_matrix","text":"evaluation_matrix(f::FEMDiscretization, X)\n\nReturn the matrix A such that A[i, j] is the value of the j-th basis function at the i-th point in X.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.node_selection_matrix","page":"Discretizations","title":"GaussianMarkovRandomFields.node_selection_matrix","text":"node_selection_matrix(f::FEMDiscretization, node_ids)\n\nReturn the matrix A such that A[i, j] = 1 if the j-th basis function is associated with the i-th node in node_ids.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.derivative_matrices","page":"Discretizations","title":"GaussianMarkovRandomFields.derivative_matrices","text":"derivative_matrices(f::FEMDiscretization{D}, X; derivative_idcs = [1])\n\nReturn a vector of matrices such that mats[k][i, j] is the derivative of the j-th basis function at X[i], where the partial derivative index is given by derivative_idcs[k].\n\nExamples\n\nWe're modelling a 2D function u(x, y) and we want the derivatives with respect to y at two input points.\n\nusing Ferrite # hide\ngrid = generate_grid(Triangle, (20,20)) # hide\nip = Lagrange{2, RefTetrahedron, 1}() # hide\nqr = QuadratureRule{2, RefTetrahedron}(2) # hide\ndisc = FEMDiscretization(grid, ip, qr)\nX = [Tensors.Vec(0.11, 0.22), Tensors.Vec(-0.1, 0.4)]\n\nmats = derivative_matrices(disc, X; derivative_idcs=[2])\n\nmats contains a single matrix of size (2, ndofs(disc)) where the i-th row contains the derivative of all basis functions with respect to y at X[i].\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.second_derivative_matrices","page":"Discretizations","title":"GaussianMarkovRandomFields.second_derivative_matrices","text":"second_derivative_matrices(f::FEMDiscretization{D}, X; derivative_idcs = [(1,1)])\n\nReturn a vector of matrices such that mats[k][i, j] is the second derivative of the j-th basis function at X[i], where the partial derivative index is given by derivative_idcs[k]. Note that the indices refer to the Hessian, i.e. (1, 2) corresponds to ∂²/∂x∂y.\n\nExamples\n\nWe're modelling a 2D function u(x, y) and we want to evaluate the Laplacian at two input points.\n\nusing Ferrite # hide\ngrid = generate_grid(Triangle, (20,20)) # hide\nip = Lagrange{2, RefTetrahedron, 1}() # hide\nqr = QuadratureRule{2, RefTetrahedron}(2) # hide\ndisc = FEMDiscretization(grid, ip, qr)\nX = [Tensors.Vec(0.11, 0.22), Tensors.Vec(-0.1, 0.4)]\n\nA, B = derivative_matrices(disc, X; derivative_idcs=[(1, 1), (2, 2)])\nlaplacian = A + B\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#Utilities","page":"Discretizations","title":"Utilities","text":"","category":"section"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.assemble_mass_matrix","page":"Discretizations","title":"GaussianMarkovRandomFields.assemble_mass_matrix","text":"assemble_mass_matrix(\n    Ce::SparseMatrixCSC,\n    cellvalues::CellValues,\n    interpolation;\n    lumping = true,\n)\n\nAssemble the mass matrix Ce for the given cell values.\n\nArguments\n\nCe::SparseMatrixCSC: The mass matrix.\ncellvalues::CellValues: Ferrite cell values.\ninterpolation::Interpolation: The interpolation scheme.\nlumping::Bool=true: Whether to lump the mass matrix.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.assemble_diffusion_matrix","page":"Discretizations","title":"GaussianMarkovRandomFields.assemble_diffusion_matrix","text":"assemble_diffusion_matrix(\n    Ge::SparseMatrixCSC,\n    cellvalues::CellValues;\n    diffusion_factor = I,\n)\n\nAssemble the diffusion matrix Ge for the given cell values.\n\nArguments\n\nGe::SparseMatrixCSC: The diffusion matrix.\ncellvalues::CellValues: Ferrite cell values.\ndiffusion_factor=I: The diffusion factor.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.assemble_advection_matrix","page":"Discretizations","title":"GaussianMarkovRandomFields.assemble_advection_matrix","text":"assemble_advection_matrix(\n    Be::SparseMatrixCSC,\n    cellvalues::CellValues;\n    advection_velocity = 1,\n)\n\nAssemble the advection matrix Be for the given cell values.\n\nArguments\n\nBe::SparseMatrixCSC: The advection matrix.\ncellvalues::CellValues: Ferrite cell values.\nadvection_velocity=1: The advection velocity.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.lump_matrix","page":"Discretizations","title":"GaussianMarkovRandomFields.lump_matrix","text":"lump_matrix(A::AbstractMatrix, ::Lagrange{D, S, 1}) where {D, S}\n\nLump a matrix by summing over the rows.\n\n\n\n\n\nlump_matrix(A::AbstractMatrix, ::Lagrange)\n\nLump a matrix through HRZ lumping. Fallback for non-linear elements. Row-summing cannot be used for non-linear elements, because it does not ensure positive definiteness.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.assemble_streamline_diffusion_matrix","page":"Discretizations","title":"GaussianMarkovRandomFields.assemble_streamline_diffusion_matrix","text":"assemble_streamline_diffusion_matrix(\n    Ge::SparseMatrixCSC,\n    cellvalues::CellValues,\n    advection_velocity,\n    h,\n)\n\nAssemble the streamline diffusion matrix Ge for the given cell values.\n\nArguments\n\nGe::SparseMatrixCSC: The streamline diffusion matrix.\ncellvalues::CellValues: Ferrite cell values.\nadvection_velocity: The advection velocity.\nh: The mesh size.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.apply_soft_constraints!","page":"Discretizations","title":"GaussianMarkovRandomFields.apply_soft_constraints!","text":"apply_soft_constraints!(K, f_rhs, ch, constraint_noise; Q_rhs = nothing, Q_rhs_sqrt = nothing)\n\nApply soft constraints to the Gaussian relation\n\nmathbfK mathbfu sim mathcalN(mathbff_textrhs mathbfQ_textrhs^-1)\n\nSoft means that the constraints are fulfilled up to noise of magnitude specified by constraint_noise.\n\nModifies K and f_rhs in place. If Q_rhs and Q_rhs_sqrt are provided, they are modified in place as well.\n\nArguments\n\nK::SparseMatrixCSC: Stiffness matrix.\nf_rhs::AbstractVector: Right-hand side.\nch::ConstraintHandler: Constraint handler.\nconstraint_noise::Vector{Float64}: Noise for each constraint.\nQ_rhs::Union{Nothing, SparseMatrixCSC}: Covariance matrix for the right-hand                                           side.\nQ_rhs_sqrt::Union{Nothing, SparseMatrixCSC}: Square root of the covariance                                                matrix for the right-hand side.\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#Temporal-discretization-and-state-space-models","page":"Discretizations","title":"Temporal discretization and state-space models","text":"","category":"section"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.JointSSMMatrices","page":"Discretizations","title":"GaussianMarkovRandomFields.JointSSMMatrices","text":"JointSSMMatrices\n\nAbstract type for the matrices defining the transition of a certain linear state-space model of the form\n\nG(Δt) x_k+1  xₖ  𝒩(M(Δt) xₖ Σ)\n\nFields\n\nΔt::Real: Time step.\nG::LinearMap: Transition matrix.\nM::LinearMap: Observation matrix.\nΣ⁻¹::LinearMap: Transition precision map.\nΣ⁻¹_sqrt::LinearMap: Square root of the transition precision map.\nconstraint_handler: Ferrite constraint handler.\nconstraint_noise: Constraint noise.\n\n\n\n\n\n","category":"type"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.joint_ssm","page":"Discretizations","title":"GaussianMarkovRandomFields.joint_ssm","text":"joint_ssm(x₀::GMRF, ssm_matrices::Function, ts::AbstractVector)\n\nForm the joint GMRF for the linear state-space model given by\n\nG(Δtₖ) x_k+1  xₖ  𝒩(M(Δtₖ) xₖ Σ)\n\nat time points given by ts (from which the Δtₖ are computed).\n\n\n\n\n\n","category":"function"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.ImplicitEulerSSM","page":"Discretizations","title":"GaussianMarkovRandomFields.ImplicitEulerSSM","text":"ImplicitEulerSSM{X,S,GF,MF,MIF,BF,BIF,TS,C,V}(\n    x₀::X,\n    G::GF,\n    M::MF,\n    M⁻¹::MIF,\n    β::BF,\n    β⁻¹::BIF,\n    spatial_noise::S,\n    ts::TS,\n    constraint_handler::C,\n    constraint_noise::V,\n)\n\nState-space model for the implicit Euler discretization of a stochastic differential equation.\n\nThe state-space model is given by\n\nG(Δt) xₖ₊₁ = M(Δt) xₖ + M(Δt) β(Δt) zₛ\n\nwhere zₛ is (possibly colored) spatial noise. \n\n\n\n\n\n","category":"type"},{"location":"reference/discretizations/#GaussianMarkovRandomFields.ImplicitEulerJointSSMMatrices","page":"Discretizations","title":"GaussianMarkovRandomFields.ImplicitEulerJointSSMMatrices","text":"ImplicitEulerJointSSMMatrices{T,GM,MM,SM,SQRT,C,V}(\n    ssm::ImplicitEulerSSM,\n    Δt::Real\n)\n\nConstruct the joint state-space model matrices for the implicit Euler discretization scheme.\n\nArguments\n\nssm::ImplicitEulerSSM: The implicit Euler state-space model.\nΔt::Real: The time step.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#GMRFs","page":"GMRFs","title":"GMRFs","text":"","category":"section"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.AbstractGMRF","page":"GMRFs","title":"GaussianMarkovRandomFields.AbstractGMRF","text":"AbstractGMRF\n\nA Gaussian Markov Random Field  (GMRF) is a special case of a multivariate normal distribution where the precision matrix is sparse. The zero entries in the precision correspond to conditional independencies.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.GMRF","page":"GMRFs","title":"GaussianMarkovRandomFields.GMRF","text":"GMRF(mean, precision, alg=LinearSolve.DefaultLinearSolver(); Q_sqrt=nothing, rbmc_strategy=RBMCStrategy(1000), linsolve_cache=nothing)\n\nA Gaussian Markov Random Field with mean mean and precision matrix precision.\n\nArguments\n\nmean::AbstractVector: The mean vector of the GMRF.\nprecision::Union{LinearMap, AbstractMatrix}: The precision matrix (inverse covariance) of the GMRF.\nalg: LinearSolve algorithm to use for linear system solving. Defaults to LinearSolve.DefaultLinearSolver().\nQ_sqrt::Union{Nothing, AbstractMatrix}: Square root of precision matrix Q, used for sampling when algorithm doesn't support backward solve.\nrbmc_strategy: RBMC strategy for marginal variance computation when selected inversion is unavailable. Defaults to RBMCStrategy(1000).\nlinsolve_cache::Union{Nothing, LinearSolve.LinearCache}: Existing LinearSolve cache to reuse. If nothing, creates a new cache. Useful for iterative algorithms requiring factorization reuse.\n\nType Parameters\n\nT<:Real: The numeric type (e.g., Float64).\nPrecisionMap<:Union{LinearMap{T}, AbstractMatrix{T}}: The type of the precision matrix.\n\nFields\n\nmean::Vector{T}: The mean vector.\nprecision::PrecisionMap: The precision matrix.\nQ_sqrt::Union{Nothing, AbstractMatrix{T}}: Square root of precision matrix for sampling.\nlinsolve_cache::LinearSolve.LinearCache: The LinearSolve cache for efficient operations.\nrbmc_strategy: RBMC strategy for variance computation fallback.\n\nNotes\n\nThe LinearSolve cache is constructed automatically (if not provided) and is used to compute means, variances,  samples, and other GMRF quantities efficiently. The algorithm choice determines which  optimization strategies (selected inversion, backward solve) are available. When selected inversion is not supported, marginal variances are computed using the configured RBMC strategy. Providing an existing linsolve_cache enables factorization reuse in iterative algorithms.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.precision_map","page":"GMRFs","title":"GaussianMarkovRandomFields.precision_map","text":"precision_map(::AbstractGMRF)\n\nReturn the precision (inverse covariance) map of the GMRF.\n\n\n\n\n\nprecision_map(d::ConstrainedGMRF)\n\nReturn the precision map of the constrained GMRF. Note: This is singular due to the constraints, but we return it for interface compliance. In practice, this should rarely be used directly due to singularity.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.precision_matrix","page":"GMRFs","title":"GaussianMarkovRandomFields.precision_matrix","text":"precision_matrix(::AbstractGMRF)\n\nReturn the precision (inverse covariance) matrix of the GMRF.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.InformationVector","page":"GMRFs","title":"GaussianMarkovRandomFields.InformationVector","text":"InformationVector(data::AbstractVector)\n\nWrapper type for information vectors (Q * μ) used in GMRF construction. This allows distinguishing between constructors that take mean vectors  vs information vectors.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.information_vector","page":"GMRFs","title":"GaussianMarkovRandomFields.information_vector","text":"information_vector(d::GMRF)\n\nReturn the information vector (Q * μ) for the GMRF. If stored, returns the cached value; otherwise computes it.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#Metadata","page":"GMRFs","title":"Metadata","text":"","category":"section"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.MetaGMRF","page":"GMRFs","title":"GaussianMarkovRandomFields.MetaGMRF","text":"MetaGMRF{M <: GMRFMetadata, T, P, G <: AbstractGMRF{T, P}} <: AbstractGMRF{T, P}\n\nA wrapper that combines a core GMRF with metadata of type M. This allows for specialized behavior based on the metadata type while preserving the computational efficiency of the underlying GMRF.\n\nFields\n\ngmrf::G: The core computational GMRF (parametric type)\nmetadata::M: Domain-specific metadata\n\nUsage\n\n# Define metadata types\nstruct SpatialMetadata <: GMRFMetadata\n    coordinates::Matrix{Float64}\n    boundary_info::Vector{Int}\nend\n\n# Create wrapped GMRF\nmeta_gmrf = MetaGMRF(my_gmrf, SpatialMetadata(coords, boundary))\n\n# Dispatch on metadata type for specialized behavior\nfunction some_spatial_operation(mgmrf::MetaGMRF{SpatialMetadata})\n    # Access coordinates via mgmrf.metadata.coordinates\n    # Access GMRF via mgmrf.gmrf\nend\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.GMRFMetadata","page":"GMRFs","title":"GaussianMarkovRandomFields.GMRFMetadata","text":"GMRFMetadata\n\nAbstract base type for metadata that can be attached to GMRFs via MetaGMRF. Concrete subtypes should contain domain-specific information about the GMRF structure, coordinates, naming, etc.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#Arithmetic","page":"GMRFs","title":"Arithmetic","text":"","category":"section"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.condition_on_observations","page":"GMRFs","title":"GaussianMarkovRandomFields.condition_on_observations","text":"\"     conditiononobservations(         x::GMRF,         A::Union{AbstractMatrix,LinearMap},         Qϵ::Union{AbstractMatrix,LinearMap,Real},         y::AbstractVector=zeros(size(A)[1]),         b::AbstractVector=zeros(size(A)[1]);         # solverblueprint parameter removed - no longer needed with LinearSolve     )\n\nCondition a GMRF x on observations y = A * x + b + ϵ where ϵ ~ N(0, Q_ϵ⁻¹).\n\nArguments\n\nx::GMRF: The GMRF to condition on.\nA::Union{AbstractMatrix,LinearMap}: The matrix A.\nQ_ϵ::Union{AbstractMatrix,LinearMap, Real}: The precision matrix of the        noise term ϵ. In case a real number is provided, it is interpreted        as a scalar multiple of the identity matrix.\ny::AbstractVector=zeros(size(A)[1]): The observations y; optional.\nb::AbstractVector=zeros(size(A)[1]): Offset vector b; optional.\n\nKeyword arguments\n\nNote: solver_blueprint parameter removed - no longer needed with LinearSolve.jl\n\nReturns\n\nA GMRF object representing the conditional GMRF x | (y = A * x + b + ϵ).\n\nNotes\n\nThis function is deprecated. Use linear_condition.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.linear_condition","page":"GMRFs","title":"GaussianMarkovRandomFields.linear_condition","text":"linear_condition(gmrf::GMRF; A, Q_ϵ, y, b=zeros(size(A, 1)))\n\nCondition a GMRF on linear observations y = A * x + b + ϵ where ϵ ~ N(0, Q_ϵ^(-1)).\n\nArguments\n\ngmrf::GMRF: The prior GMRF\nA::Union{AbstractMatrix, LinearMap}: Observation matrix\nQ_ϵ::Union{AbstractMatrix, LinearMap}: Precision matrix of observation noise\ny::AbstractVector: Observation values\nb::AbstractVector: Offset vector (defaults to zeros)\n\nReturns\n\nA new GMRF representing the posterior distribution with updated mean and precision.\n\nNotes\n\nUses information vector arithmetic for efficient conditioning without intermediate solves.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.joint_gmrf","page":"GMRFs","title":"GaussianMarkovRandomFields.joint_gmrf","text":"\"     jointgmrf(         x1::AbstractGMRF,         A::AbstractMatrix,         Qϵ::AbstractMatrix,         b::AbstractVector=spzeros(size(A)[1])     )\n\nReturn the joint GMRF of x1 and x2 = A * x1 + b + ϵ where ϵ ~ N(0, Q_ϵ⁻¹).\n\nArguments\n\nx1::AbstractGMRF: The first GMRF.\nA::AbstractMatrix: The matrix A.\nQ_ϵ::AbstractMatrix: The precision matrix of the noise term ϵ.\nb::AbstractVector=spzeros(size(A)[1]): Offset vector b; optional.\n\nReturns\n\nA GMRF object representing the joint GMRF of x1 and x2 = A * x1 + b + ϵ.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#Spatiotemporal-setting","page":"GMRFs","title":"Spatiotemporal setting","text":"","category":"section"},{"location":"reference/gmrfs/#Types","page":"GMRFs","title":"Types","text":"","category":"section"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.AbstractSpatiotemporalGMRF","page":"GMRFs","title":"GaussianMarkovRandomFields.AbstractSpatiotemporalGMRF","text":"AbstractSpatiotemporalGMRF\n\nA spatiotemporal GMRF is a GMRF that explicitly encodes the spatial and temporal structure of the underlying random field. All time points are modelled in one joint GMRF. It provides utilities to get statistics, draw samples and get the spatial discretization at a given time.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.ImplicitEulerConstantMeshSTGMRF","page":"GMRFs","title":"GaussianMarkovRandomFields.ImplicitEulerConstantMeshSTGMRF","text":"ImplicitEulerConstantMeshSTGMRF\n\nA spatiotemporal GMRF with constant spatial discretization and an implicit Euler discretization of the temporal dynamics. Uses MetaGMRF for clean type structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.ConcreteConstantMeshSTGMRF","page":"GMRFs","title":"GaussianMarkovRandomFields.ConcreteConstantMeshSTGMRF","text":"ConcreteConstantMeshSTGMRF\n\nA concrete implementation of a spatiotemporal GMRF with constant spatial discretization. Uses MetaGMRF for clean type structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/gmrfs/#Quantities","page":"GMRFs","title":"Quantities","text":"","category":"section"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.N_t","page":"GMRFs","title":"GaussianMarkovRandomFields.N_t","text":"N_t(::AbstractSpatiotemporalGMRF)\n\nReturn the number of time points in the spatiotemporal GMRF.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.time_means","page":"GMRFs","title":"GaussianMarkovRandomFields.time_means","text":"time_means(::AbstractSpatiotemporalGMRF)\n\nReturn the means of the spatiotemporal GMRF at each time point.\n\nReturns\n\nA vector of means of length Nₜ, one for each time point.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.time_vars","page":"GMRFs","title":"GaussianMarkovRandomFields.time_vars","text":"time_vars(::AbstractSpatiotemporalGMRF)\n\nReturn the marginal variances of the spatiotemporal GMRF at each time point.\n\nReturns\n\nA vector of marginal variances of length Nₜ, one for each time point.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.time_stds","page":"GMRFs","title":"GaussianMarkovRandomFields.time_stds","text":"time_stds(::AbstractSpatiotemporalGMRF)\n\nReturn the marginal standard deviations of the spatiotemporal GMRF at each time point.\n\nReturns\n\nA vector of marginal standard deviations of length Nₜ, one for each time point.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.time_rands","page":"GMRFs","title":"GaussianMarkovRandomFields.time_rands","text":"time_rands(::AbstractSpatiotemporalGMRF, rng::AbstractRNG)\n\nDraw samples from the spatiotemporal GMRF at each time point.\n\nReturns\n\nA vector of sample values of length Nₜ, one sample value vector for each time point.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.discretization_at_time","page":"GMRFs","title":"GaussianMarkovRandomFields.discretization_at_time","text":"discretization_at_time(::AbstractSpatiotemporalGMRF, t::Int)\n\nReturn the spatial discretization at time t.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#Utilities","page":"GMRFs","title":"Utilities","text":"","category":"section"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.spatial_to_spatiotemporal","page":"GMRFs","title":"GaussianMarkovRandomFields.spatial_to_spatiotemporal","text":"spatial_to_spatiotemporal(\n    spatial_matrix::AbstractMatrix,\n    t_idx::Int,\n    N_t::Int,\n)\n\nMake a spatial matrix applicable to a spatiotemporal system at time index t_idx. Results in a matrix that selects the spatial information exactly at time t_idx.\n\nArguments\n\nspatial_matrix::AbstractMatrix: The spatial matrix.\nt_idx::Integer: The time index.\nN_t::Integer: The number of time points.\n\n\n\n\n\n","category":"function"},{"location":"reference/gmrfs/#GaussianMarkovRandomFields.kronecker_product_spatiotemporal_model","page":"GMRFs","title":"GaussianMarkovRandomFields.kronecker_product_spatiotemporal_model","text":"kronecker_product_spatiotemporal_model(\n    Q_t::AbstractMatrix,\n    Q_s::AbstractMatrix,\n    spatial_disc::FEMDiscretization;\n    algorithm = nothing,\n)\n\nCreate a spatiotemporal GMRF through a Kronecker product of the temporal and spatial precision matrices.\n\nArguments\n\nQ_t::AbstractMatrix: The temporal precision matrix.\nQ_s::AbstractMatrix: The spatial precision matrix.\nspatial_disc::FEMDiscretization: The spatial discretization.\n\nKeyword arguments\n\nalgorithm: The LinearSolve algorithm to use.\n\n\n\n\n\n","category":"function"},{"location":"dev-docs/#Developer-documentation-overview","page":"Overview","title":"Developer documentation overview","text":"","category":"section"},{"location":"dev-docs/","page":"Overview","title":"Overview","text":"Pages = [\n    \"spdes.md\",\n    \"discretizations.md\",\n    \"solvers.md\"\n]","category":"page"},{"location":"reference/observation_models/#Observation-Models","page":"Observation Models","title":"Observation Models","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"Observation models define the relationship between observations y and the latent GMRF field x, typically through likelihood functions. They enable Bayesian inference by connecting your data to the underlying Gaussian process through flexible probabilistic models.","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"GaussianMarkovRandomFields.jl implements observation models using a factory pattern that separates model configuration from materialized evaluation instances. This design provides major performance benefits in optimization loops and cleaner automatic differentiation boundaries.","category":"page"},{"location":"reference/observation_models/#Core-Concepts","page":"Observation Models","title":"Core Concepts","text":"","category":"section"},{"location":"reference/observation_models/#The-Factory-Pattern","page":"Observation Models","title":"The Factory Pattern","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"Observation models follow a two-stage pattern:","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"ObservationModel: A factory that defines the model structure and hyperparameters\nObservationLikelihood: A materialized instance with specific data and hyperparameters for fast evaluation","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Step 1: Configure observation model (factory)\nobs_model = ExponentialFamily(Normal)\n\n# Step 2: Materialize with data and hyperparameters  \nobs_lik = obs_model(y; σ=1.2)\n\n# Step 3: Fast evaluation in hot loops\nll = loglik(x, obs_lik)      # Only x argument needed!\ngrad = loggrad(x, obs_lik)   # Fast x-only evaluation\nhess = loghessian(x, obs_lik)","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"This pattern eliminates the need to repeatedly pass data and hyperparameters, providing significant performance benefits in optimization and sampling algorithms.","category":"page"},{"location":"reference/observation_models/#Evaluation-Interface","page":"Observation Models","title":"Evaluation Interface","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"All materialized observation likelihoods support a common interface:","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"loglik(x, obs_lik): Evaluate log-likelihood \nloggrad(x, obs_lik): Compute gradient with respect to latent field\nloghessian(x, obs_lik): Compute Hessian matrix","category":"page"},{"location":"reference/observation_models/#Exponential-Family-Models","page":"Observation Models","title":"Exponential Family Models","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"The most common observation models are exponential family distributions connected to the latent field through link functions.","category":"page"},{"location":"reference/observation_models/#Basic-Usage","page":"Observation Models","title":"Basic Usage","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"using GaussianMarkovRandomFields\nusing Distributions\n\n# Poisson model for count data (canonical LogLink)\npoisson_model = ExponentialFamily(Poisson)\nx = [1.0, 2.0]  # Latent field (log-intensity due to LogLink)\ny = [2, 7]      # Count observations\nobs_lik = poisson_model(y)\nll = loglik(x, obs_lik)\n\n# Normal model for continuous data (canonical IdentityLink)\nnormal_model = ExponentialFamily(Normal)\nx = [1.5, 2.3]  # Latent field (direct mean due to IdentityLink)\ny = [1.2, 2.8]  # Continuous observations\nobs_lik = normal_model(y; σ=0.5)  # Normal requires σ hyperparameter\nll = loglik(x, obs_lik)\n\n# Bernoulli model for binary data (canonical LogitLink)\nbernoulli_model = ExponentialFamily(Bernoulli)\nx = [0.0, 1.5]  # Latent field (logit-probability due to LogitLink)\ny = [0, 1]      # Binary observations\nobs_lik = bernoulli_model(y)\nll = loglik(x, obs_lik)","category":"page"},{"location":"reference/observation_models/#Supported-Distributions-and-Links","page":"Observation Models","title":"Supported Distributions and Links","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"Distribution Canonical Link Alternative Links Hyperparameters\nNormal IdentityLink LogLink σ (std. dev.)\nPoisson LogLink IdentityLink none\nBernoulli LogitLink LogLink none\nBinomial LogitLink IdentityLink none*","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"*For Binomial, the number of trials is provided through the data structure BinomialObservations, not as a hyperparameter.","category":"page"},{"location":"reference/observation_models/#Custom-Link-Functions","page":"Observation Models","title":"Custom Link Functions","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Non-canonical link function\npoisson_identity = ExponentialFamily(Poisson, IdentityLink())\n# Note: Requires positive latent field values for valid Poisson intensities","category":"page"},{"location":"reference/observation_models/#Custom-Observation-Models","page":"Observation Models","title":"Custom Observation Models","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"For models not covered by exponential families, you can define custom log-likelihood functions using automatic differentiation.","category":"page"},{"location":"reference/observation_models/#Basic-AutoDiff-Models","page":"Observation Models","title":"Basic AutoDiff Models","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Define custom log-likelihood function\nfunction custom_loglik(x; y=[1.0, 2.0], σ=1.0)\n    μ = sin.(x)  # Custom transformation\n    return -0.5 * sum((y .- μ).^2) / σ^2 - length(y) * log(σ)\nend\n\n# Create observation model\nobs_model = AutoDiffObservationModel(custom_loglik; n_latent=2, hyperparams=(:y, :σ))\n\n# Materialize with data\nobs_lik = obs_model(y=[1.2, 1.8], σ=0.5)\n\n# Use normally - gradients and Hessians computed automatically!\nx = [0.5, 1.0]\nll = loglik(x, obs_lik)\ngrad = loggrad(x, obs_lik)    # Automatic differentiation\nhess = loghessian(x, obs_lik) # Potentially sparse!","category":"page"},{"location":"reference/observation_models/#Automatic-Differentiation-Requirements","page":"Observation Models","title":"Automatic Differentiation Requirements","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"AutoDiff observation models require an automatic differentiation backend. We support and recommend the following backends in order of preference:","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"Enzyme.jl (recommended for performance)\nMooncake.jl (good balance of performance and compatibility)\nZygote.jl (reliable fallback)\nForwardDiff.jl (for small problems)","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Load an AD backend (required for AutoDiffObservationModel)\nusing Enzyme  # Recommended\n\n# Or use another supported backend:\n# using Mooncake\n# using Zygote\n# using ForwardDiff\n\n# Now you can use AutoDiff models\nobs_model = AutoDiffObservationModel(my_loglik; n_latent=10)\nobs_lik = obs_model(y=data)\ngrad = loggrad(x, obs_lik)  # Uses your loaded AD backend","category":"page"},{"location":"reference/observation_models/#Sparse-Hessian-Computation","page":"Observation Models","title":"Sparse Hessian Computation","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"AutoDiff observation models can automatically detect and exploit sparsity in Hessian matrices using our package extensions. This requires loading both an AD backend and additional sparsity packages:","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Load AD backend + sparse AD packages\nusing Enzyme  # Or your preferred AD backend\nusing SparseConnectivityTracer, SparseMatrixColorings\n\n# The package extension is automatically activated\nobs_model = AutoDiffObservationModel(my_loglik; n_latent=100)\nobs_lik = obs_model(y=data)\n\n# Hessian computation now automatically:\n# - Detects sparsity pattern using TracerSparsityDetector  \n# - Uses greedy coloring for efficient computation\n# - Returns sparse matrix when beneficial\nhess = loghessian(x, obs_lik)  # May be sparse!","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"The sparse Hessian features provide dramatic performance improvements for large-scale problems with structured sparsity.","category":"page"},{"location":"reference/observation_models/#Advanced-Features","page":"Observation Models","title":"Advanced Features","text":"","category":"section"},{"location":"reference/observation_models/#Linear-Transformations-and-Design-Matrices","page":"Observation Models","title":"Linear Transformations and Design Matrices","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"For GLM-style modeling where observations are related to linear combinations of latent field components:","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Design matrix mapping latent field to linear predictors\n# Rows = observations, Columns = latent components\nA = [1.0  20.0  1.0  0.0;   # obs 1: intercept + temp + group1\n     1.0  25.0  1.0  0.0;   # obs 2: intercept + temp + group1  \n     1.0  30.0  0.0  1.0]   # obs 3: intercept + temp + group2\n\nbase_model = ExponentialFamily(Poisson)  # LogLink by default\nobs_model = LinearlyTransformedObservationModel(base_model, A)\n\n# Latent field now includes all components: [β₀, β₁, u₁, u₂]\nx_full = [2.0, 0.1, -0.5, 0.3]  # intercept, slope, group effects\nobs_lik = obs_model(y)\nll = loglik(x_full, obs_lik)  # Chain rule applied automatically","category":"page"},{"location":"reference/observation_models/#Binomial-Observations","page":"Observation Models","title":"Binomial Observations","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"For binomial data, use the BinomialObservations utility:","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Create binomial observations with successes and trials\ny = BinomialObservations([3, 1, 4], [5, 8, 6])  # (successes, trials) pairs\n\n# Use with Binomial model\nbinomial_model = ExponentialFamily(Binomial) \nobs_lik = binomial_model(y)\n\n# Access components\nsuccesses(y)  # [3, 1, 4]\ntrials(y)     # [5, 8, 6]\ny[1]          # (3, 5) - tuple access","category":"page"},{"location":"reference/observation_models/#Composite-Observations","page":"Observation Models","title":"Composite Observations","text":"","category":"section"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"For multiple observation types in a single model:","category":"page"},{"location":"reference/observation_models/","page":"Observation Models","title":"Observation Models","text":"# Multiple observation vectors\ncount_data = [1, 3, 0, 2]\nbinary_data = [0, 1, 1, 0]\nobs = CompositeObservations(count_data, binary_data)\n\n# Corresponding models for each observation type\npoisson_model = ExponentialFamily(Poisson)\nbernoulli_model = ExponentialFamily(Bernoulli) \ncomposite_model = CompositeObservationModel(poisson_model, bernoulli_model)\n\nobs_lik = composite_model(obs)\n# Latent field x now corresponds to concatenated observations\nll = loglik(x, obs_lik)","category":"page"},{"location":"reference/observation_models/#API-Reference","page":"Observation Models","title":"API Reference","text":"","category":"section"},{"location":"reference/observation_models/#Core-Types-and-Interface","page":"Observation Models","title":"Core Types and Interface","text":"","category":"section"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.ObservationModel","page":"Observation Models","title":"GaussianMarkovRandomFields.ObservationModel","text":"ObservationModel\n\nAbstract base type for all observation models for GMRFs.\n\nAn observation model defines the relationship between observations y and the latent field x, typically through a likelihood function. ObservationModel types serve as factories for creating ObservationLikelihood instances via callable syntax.\n\nUsage Pattern\n\n# Step 1: Create observation model (factory)\nobs_model = ExponentialFamily(Normal)\n\n# Step 2: Materialize with data and hyperparameters\nobs_lik = obs_model(y; σ=1.2)  # Creates ObservationLikelihood\n\n# Step 3: Use materialized likelihood in hot loops\nll = loglik(x, obs_lik)  # Fast x-only evaluation\n\nSee also: ObservationLikelihood, ExponentialFamily\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.ObservationLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.ObservationLikelihood","text":"ObservationLikelihood\n\nAbstract base type for materialized observation likelihoods.\n\nObservation likelihoods are created by materializing an observation model with specific hyperparameters θ and observed data y. They provide efficient evaluation methods that  only depend on the latent field x, eliminating the need to repeatedly pass θ and y.\n\nThis design provides major performance benefits in optimization loops and cleaner  automatic differentiation boundaries.\n\nUsage Pattern\n\n# Step 1: Configure observation model (factory)\nobs_model = ExponentialFamily(Normal)\n\n# Step 2: Materialize with data and hyperparameters  \nobs_lik = obs_model(y; σ=1.2)\n\n# Step 3: Fast evaluation in hot loops\nll = loglik(x, obs_lik)      # Only x argument needed!\ngrad = loggrad(x, obs_lik)   # Fast x-only evaluation\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.hyperparameters","page":"Observation Models","title":"GaussianMarkovRandomFields.hyperparameters","text":"hyperparameters(obs_model::ObservationModel) -> Tuple{Vararg{Symbol}}\n\nReturn a tuple of required hyperparameter names for this observation model.\n\nThis method defines which hyperparameters the observation model expects to receive when materializing an ObservationLikelihood instance.\n\nArguments\n\nobs_model: An observation model implementing the ObservationModel interface\n\nReturns\n\nTuple{Vararg{Symbol}}: Tuple of parameter names (e.g., (:σ,) or (:α, :β))\n\nExample\n\nhyperparameters(ExponentialFamily(Normal)) == (:σ,)\nhyperparameters(ExponentialFamily(Bernoulli)) == ()\n\nImplementation\n\nAll observation models should implement this method. The default returns an empty tuple.\n\n\n\n\n\nhyperparameters(ltom::LinearlyTransformedObservationModel) -> Tuple{Vararg{Symbol}}\n\nReturn required hyperparameters by delegating to the base model.\n\nThe design matrix introduces no new hyperparameters - all parameters come from  the base observation model.\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.latent_dimension","page":"Observation Models","title":"GaussianMarkovRandomFields.latent_dimension","text":"latent_dimension(obs_model::ObservationModel, y::AbstractVector) -> Union{Int, Nothing}\n\nReturn the latent field dimension for this observation model given observations y.\n\nFor most observation models, this will be length(y) (1:1 mapping). For transformed observation models like LinearlyTransformedObservationModel, this will be the dimension of the design matrix.\n\nReturns nothing if the latent dimension cannot be determined automatically.\n\nArguments\n\nobs_model: An observation model implementing the ObservationModel interface\ny: Vector of observations\n\nReturns\n\nInt: The latent field dimension, or nothing if unknown\n\nExample\n\nlatent_dimension(ExponentialFamily(Normal), y) == length(y)\nlatent_dimension(LinearlyTransformedObservationModel(base, A), y) == size(A, 2)\n\n\n\n\n\nlatent_dimension(ef::ExponentialFamily, y::AbstractVector) -> Int\n\nReturn the latent field dimension for exponential family models.\n\nFor ExponentialFamily models, there is a direct 1:1 mapping between observations and latent field components, so the latent dimension equals the observation dimension.\n\n\n\n\n\nlatent_dimension(ltom::LinearlyTransformedObservationModel, y::AbstractVector) -> Int\n\nReturn the latent field dimension for a linearly transformed observation model.\n\nThe latent dimension is the number of columns in the design matrix, representing the dimension of the full latent field (not the linear predictors).\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.loglik","page":"Observation Models","title":"GaussianMarkovRandomFields.loglik","text":"loglik(x, lik::ExponentialFamilyLikelihood) -> Float64\n\nGeneric loglik implementation for all exponential family likelihoods using product_distribution.\n\n\n\n\n\nloglik(x, lik::NormalLikelihood) -> Float64\n\nSpecialized fast implementation for Normal likelihood that avoids product_distribution overhead.\n\nComputes: ∑ᵢ logpdf(Normal(μᵢ, σ), yᵢ) = -n/2 * log(2π) - n * log(σ) - 1/(2σ²) * ∑ᵢ(yᵢ - μᵢ)²\n\n\n\n\n\nloglik(x, composite_lik::CompositeLikelihood) -> Float64\n\nCompute the log-likelihood of a composite likelihood by summing component contributions.\n\nEach component likelihood receives the full latent field x and contributes to the total log-likelihood. This handles cases where components may have overlapping dependencies on the latent field.\n\n\n\n\n\nloglik(x_full, ltlik::LinearlyTransformedLikelihood) -> Float64\n\nEvaluate log-likelihood for materialized LinearlyTransformedLikelihood.\n\nThis is the performance-critical method used in optimization loops. It applies the  linear transformation η = A * x_full and delegates to the base likelihood.\n\nArguments\n\nx_full: Full latent field vector (length: nlatentcomponents)\nltlik: MaterializedLinearlyTransformedLikelihood instance  \n\nReturns\n\nFloat64: Log-likelihood value\n\nMathematical Details\n\nComputes: log p(y | η, θ) where η = A * x_full\n\n\n\n\n\nloglik(x, obs_lik::AutoDiffLikelihood) -> Real\n\nEvaluate the log-likelihood function at latent field x.\n\nCalls the stored log-likelihood function, which typically includes all necessary hyperparameters and data as a closure.\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.loggrad","page":"Observation Models","title":"GaussianMarkovRandomFields.loggrad","text":"loggrad(x, obs_lik::ObservationLikelihood) -> Vector{Float64}\n\nAutomatic differentiation fallback for ObservationLikelihood gradient computation.\n\n\n\n\n\nloggrad(x, composite_lik::CompositeLikelihood) -> Vector{Float64}\n\nCompute the gradient of the log-likelihood by summing component gradients.\n\nEach component contributes its gradient with respect to the full latent field x. For overlapping dependencies, gradients are automatically summed at each latent field element.\n\n\n\n\n\nloggrad(x_full, ltlik::LinearlyTransformedLikelihood) -> Vector{Float64}\n\nCompute gradient of log-likelihood with respect to full latent field using chain rule.\n\nApplies the chain rule: ∇{xfull} ℓ = A^T ∇η ℓ where A is the design matrix and ∇η ℓ is the gradient computed by the base likelihood.\n\nArguments\n\nx_full: Full latent field vector (length: nlatentcomponents)\nltlik: MaterializedLinearlyTransformedLikelihood instance\n\nReturns\n\nVector{Float64}: Gradient vector of same length as x_full\n\nMathematical Details\n\nTransform: η = A * x_full\nCompute base gradient: gradη = ∇η log p(y | η, θ)  \nApply chain rule: gradx = A^T * gradη\n\nPerformance Notes\n\nPreserves sparsity if A is sparse\nEfficiently handles both dense and sparse design matrices\nReuses optimized gradient computations from base likelihood\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.loghessian","page":"Observation Models","title":"GaussianMarkovRandomFields.loghessian","text":"loghessian(x, obs_lik::ObservationLikelihood) -> AbstractMatrix{Float64}\n\nAutomatic differentiation fallback for ObservationLikelihood Hessian computation.\n\n\n\n\n\nloghessian(x, composite_lik::CompositeLikelihood) -> AbstractMatrix{Float64}\n\nCompute the Hessian of the log-likelihood by summing component Hessians.\n\nEach component contributes its Hessian with respect to the full latent field x. For overlapping dependencies, Hessians are automatically summed element-wise.\n\n\n\n\n\nloghessian(x_full, ltlik::LinearlyTransformedLikelihood) -> AbstractMatrix{Float64}\n\nCompute Hessian of log-likelihood with respect to full latent field using chain rule.\n\nApplies the chain rule for Hessians: ∇²{xfull} ℓ = A^T ∇²η ℓ A where A is the  design matrix and ∇²η ℓ is the Hessian computed by the base likelihood.\n\nArguments\n\nx_full: Full latent field vector (length: nlatentcomponents)\nltlik: MaterializedLinearlyTransformedLikelihood instance\n\nReturns\n\nAbstractMatrix{Float64}: Hessian matrix of size (nlatentcomponents, nlatentcomponents)\n\nMathematical Details\n\nTransform: η = A * x_full\nCompute base Hessian: hessη = ∇²η log p(y | η, θ)\nApply chain rule: hessx = A^T * hessη * A\n\nPerformance Notes\n\nSparsity preservation: If A and hess_η are sparse, result will be sparse\nEfficient sandwich computation: Uses optimized matrix multiplication\nMemory scaling: Result size is (nlatent, nlatent), larger than base (nobs, nobs)\nFill-in warning: A^T * hess_η * A can create new non-zeros even if inputs are sparse\n\nCommon Case Optimization\n\nFor diagonal base Hessians (common with canonical links), the computation A^T * Diagonal(d) * A can be computed efficiently without forming the full diagonal matrix.\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#Exponential-Family-Models-2","page":"Observation Models","title":"Exponential Family Models","text":"","category":"section"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.ExponentialFamily","page":"Observation Models","title":"GaussianMarkovRandomFields.ExponentialFamily","text":"ExponentialFamily{F<:Distribution, L<:LinkFunction} <: ObservationModel\n\nObservation model for exponential family distributions with link functions.\n\nThis struct represents observation models where the observations come from an exponential  family distribution (Normal, Poisson, Bernoulli, Binomial) and the mean parameter is  related to the latent field through a link function.\n\nMathematical Model\n\nFor observations yᵢ and latent field values xᵢ:\n\nLinear predictor: ηᵢ = xᵢ\nMean parameter: μᵢ = g⁻¹(ηᵢ) where g is the link function\nObservations: yᵢ ~ F(μᵢ, θ) where F is the distribution family\n\nFields\n\nfamily::Type{F}: The distribution family (e.g., Poisson, Bernoulli)\nlink::L: The link function connecting mean parameters to linear predictors\n\nType Parameters\n\nF: A subtype of Distribution from Distributions.jl\nL: A subtype of LinkFunction\n\nConstructors\n\n# Use canonical link (recommended)\nExponentialFamily(Poisson)        # Uses LogLink()\nExponentialFamily(Bernoulli)      # Uses LogitLink()\nExponentialFamily(Normal)         # Uses IdentityLink()\n\n# Specify custom link function\nExponentialFamily(Poisson, IdentityLink())  # Non-canonical\n\nSupported Combinations\n\nNormal with IdentityLink (canonical) or LogLink\nPoisson with LogLink (canonical) or IdentityLink  \nBernoulli with LogitLink (canonical) or LogLink\nBinomial with LogitLink (canonical) or IdentityLink\n\nHyperparameters (θ)\n\nDifferent families require different hyperparameters:\n\nNormal: θ = [σ] (standard deviation)\nPoisson: θ = [] (no hyperparameters)\nBernoulli: θ = [] (no hyperparameters)\nBinomial: θ = [n] (number of trials)\n\nExamples\n\n# Poisson model for count data\nmodel = ExponentialFamily(Poisson)\nx = [1.0, 2.0]        # Latent field (log scale due to LogLink)\nθ = Float64[]         # No hyperparameters  \ny = [2, 7]           # Count observations\n\nll = loglik(x, model, θ, y)\ndist = data_distribution(model, x, θ)  # Returns Product distribution\n\n# Bernoulli model for binary data\nmodel = ExponentialFamily(Bernoulli)\nx = [0.0, 1.0]       # Latent field (logit scale due to LogitLink)\ny = [0, 1]           # Binary observations\n\nPerformance Notes\n\nCanonical link functions have optimized implementations that avoid redundant computations. Non-canonical links use general chain rule formulations which may be slower.\n\nSee also: LinkFunction, loglik, data_distribution\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.data_distribution","page":"Observation Models","title":"GaussianMarkovRandomFields.data_distribution","text":"data_distribution(obs_model::ExponentialFamily, x, θ_named) -> Distribution\n\nConstruct the data-generating distribution p(y | x, θ).\n\nThis function returns a Distribution object that represents the probability  distribution over observations y given latent field values x and hyperparameters θ. It is used for sampling new observations.\n\nArguments\n\nobs_model: An ExponentialFamily observation model\nx: Latent field values (vector)  \nθ_named: Hyperparameters as a NamedTuple\n\nReturns\n\nDistribution object that can be used with rand() to generate observations\n\nExample\n\nmodel = ExponentialFamily(Poisson)\nx = [1.0, 2.0]\nθ_named = NamedTuple()\ndist = data_distribution(model, x, θ_named)\ny = rand(dist)  # Sample observations\n\nNote: For likelihood evaluation L(x|y,θ), use the materialized API:\n\nobs_lik = obs_model(y; θ_named...)\nll = loglik(x, obs_lik)\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.ExponentialFamilyLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.ExponentialFamilyLikelihood","text":"ExponentialFamilyLikelihood{L, I} <: ObservationLikelihood\n\nAbstract type for exponential family observation likelihoods.\n\nThis intermediate type allows for generic implementations that work across all  exponential family distributions while still allowing specialized methods for  specific combinations.\n\nType Parameters\n\nL: Link function type\nI: Index type (Nothing for non-indexed, UnitRange or Vector for indexed)\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.NormalLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.NormalLikelihood","text":"NormalLikelihood{L<:LinkFunction} <: ObservationLikelihood\n\nMaterialized Normal observation likelihood with precomputed hyperparameters.\n\nFields\n\nlink::L: Link function connecting latent field to mean parameter\ny::Vector{Float64}: Observed data  \nσ::Float64: Standard deviation hyperparameter\ninv_σ²::Float64: Precomputed 1/σ² for performance\nlog_σ::Float64: Precomputed log(σ) for log-likelihood computation\n\nExample\n\nobs_model = ExponentialFamily(Normal)\nobs_lik = obs_model([1.0, 2.0, 1.5]; σ=0.5)  # NormalLikelihood{IdentityLink}\nll = loglik([0.9, 2.1, 1.4], obs_lik)\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.PoissonLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.PoissonLikelihood","text":"PoissonLikelihood{L<:LinkFunction} <: ObservationLikelihood\n\nMaterialized Poisson observation likelihood.\n\nFields\n\nlink::L: Link function connecting latent field to rate parameter\ny::Vector{Int}: Count observations\n\nExample\n\nobs_model = ExponentialFamily(Poisson)  # Uses LogLink by default\nobs_lik = obs_model([1, 3, 0, 2])      # PoissonLikelihood{LogLink}\nll = loglik([0.0, 1.1, -2.0, 0.7], obs_lik)  # x values on log scale\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.BernoulliLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.BernoulliLikelihood","text":"BernoulliLikelihood{L<:LinkFunction} <: ObservationLikelihood\n\nMaterialized Bernoulli observation likelihood for binary data.\n\nFields\n\nlink::L: Link function connecting latent field to probability parameter  \ny::Vector{Int}: Binary observations (0 or 1)\n\nExample\n\nobs_model = ExponentialFamily(Bernoulli)  # Uses LogitLink by default\nobs_lik = obs_model([1, 0, 1, 0])        # BernoulliLikelihood{LogitLink}\nll = loglik([0.5, -0.2, 1.1, -0.8], obs_lik)  # x values on logit scale\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.BinomialLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.BinomialLikelihood","text":"BinomialLikelihood{L<:LinkFunction} <: ObservationLikelihood\n\nMaterialized Binomial observation likelihood.\n\nFields\n\nlink::L: Link function connecting latent field to probability parameter\ny::Vector{Int}: Number of successes for each trial\nn::Vector{Int}: Number of trials per observation (can vary across observations)\n\nExample\n\nobs_model = ExponentialFamily(Binomial)  # Uses LogitLink by default\nobs_lik = obs_model([3, 1, 4]; trials=[5, 8, 6])  # BinomialLikelihood{LogitLink}\nll = loglik([0.2, -1.0, 0.8], obs_lik)  # x values on logit scale\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#Link-Functions","page":"Observation Models","title":"Link Functions","text":"","category":"section"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.LinkFunction","page":"Observation Models","title":"GaussianMarkovRandomFields.LinkFunction","text":"LinkFunction\n\nAbstract base type for link functions used in exponential family models.\n\nA link function g(μ) connects the mean parameter μ of a distribution to the linear  predictor η through the relationship g(μ) = η, or equivalently μ = g⁻¹(η).\n\nImplemented Link Functions\n\nIdentityLink: g(μ) = μ (for Normal distributions)\nLogLink: g(μ) = log(μ) (for Poisson distributions)  \nLogitLink: g(μ) = logit(μ) (for Bernoulli/Binomial distributions)\n\nInterface\n\nConcrete link functions must implement:\n\napply_link(link, μ): Apply the link function g(μ)\napply_invlink(link, η): Apply the inverse link function g⁻¹(η)\n\nFor performance in GMRF computations, they should also implement:\n\nderivative_invlink(link, η): First derivative of g⁻¹(η)\nsecond_derivative_invlink(link, η): Second derivative of g⁻¹(η)\n\nSee also: ExponentialFamily, apply_link, apply_invlink\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.IdentityLink","page":"Observation Models","title":"GaussianMarkovRandomFields.IdentityLink","text":"IdentityLink <: LinkFunction\n\nIdentity link function: g(μ) = μ.\n\nThis is the canonical link for Normal distributions. The mean parameter μ is  directly equal to the linear predictor η.\n\nMathematical Definition\n\nLink: g(μ) = μ\nInverse link: g⁻¹(η) = η  \nFirst derivative: d/dη g⁻¹(η) = 1\nSecond derivative: d²/dη² g⁻¹(η) = 0\n\nExample\n\nlink = IdentityLink()\nμ = apply_invlink(link, 1.5)  # μ = 1.5\nη = apply_link(link, μ)       # η = 1.5\n\nSee also: LogLink, LogitLink\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.LogLink","page":"Observation Models","title":"GaussianMarkovRandomFields.LogLink","text":"LogLink <: LinkFunction\n\nLogarithmic link function: g(μ) = log(μ).\n\nThis is the canonical link for Poisson and Gamma distributions. It ensures the  mean parameter μ remains positive by mapping the real-valued linear predictor η  to μ = exp(η).\n\nMathematical Definition\n\nLink: g(μ) = log(μ) \nInverse link: g⁻¹(η) = exp(η)\nFirst derivative: d/dη g⁻¹(η) = exp(η)\nSecond derivative: d²/dη² g⁻¹(η) = exp(η)\n\nExample\n\nlink = LogLink()\nμ = apply_invlink(link, 1.0)  # μ = exp(1.0) ≈ 2.718\nη = apply_link(link, μ)       # η = log(μ) = 1.0\n\nSee also: IdentityLink, LogitLink\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.LogitLink","page":"Observation Models","title":"GaussianMarkovRandomFields.LogitLink","text":"LogitLink <: LinkFunction\n\nLogit link function: g(μ) = logit(μ) = log(μ/(1-μ)).\n\nThis is the canonical link for Bernoulli and Binomial distributions. It maps  probabilities μ ∈ (0,1) to the real line via the logistic transformation,  ensuring μ = logistic(η) = 1/(1+exp(-η)) remains a valid probability.\n\nMathematical Definition\n\nLink: g(μ) = logit(μ) = log(μ/(1-μ))\nInverse link: g⁻¹(η) = logistic(η) = 1/(1+exp(-η))\nFirst derivative: d/dη g⁻¹(η) = μ(1-μ) where μ = logistic(η)\nSecond derivative: d²/dη² g⁻¹(η) = μ(1-μ)(1-2μ)\n\nExample\n\nlink = LogitLink()\nμ = apply_invlink(link, 0.0)  # μ = logistic(0.0) = 0.5\nη = apply_link(link, μ)       # η = logit(0.5) = 0.0\n\nSee also: IdentityLink, LogLink\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.apply_link","page":"Observation Models","title":"GaussianMarkovRandomFields.apply_link","text":"apply_link(link::LinkFunction, μ) -> Real\n\nApply the link function g(μ) to transform mean parameters to linear predictor scale.\n\nThis function computes η = g(μ), where g is the link function. This transformation is typically used to ensure the mean parameter satisfies appropriate constraints (e.g., positivity for Poisson, probability bounds for Bernoulli).\n\nArguments\n\nlink: A link function (IdentityLink, LogLink, or LogitLink)\nμ: Mean parameter value(s) in the natural parameter space\n\nReturns\n\nThe transformed value(s) η on the linear predictor scale\n\nExamples\n\napply_link(LogLink(), 2.718)      # ≈ 1.0\napply_link(LogitLink(), 0.5)      # = 0.0  \napply_link(IdentityLink(), 1.5)   # = 1.5\n\nSee also: apply_invlink, LinkFunction\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.apply_invlink","page":"Observation Models","title":"GaussianMarkovRandomFields.apply_invlink","text":"apply_invlink(link::LinkFunction, η) -> Real\n\nApply the inverse link function g⁻¹(η) to transform linear predictor to mean parameters.\n\nThis function computes μ = g⁻¹(η), where g⁻¹ is the inverse link function. This is the primary transformation used in GMRF models to convert the latent field values to the natural parameter space of the observation distribution.\n\nArguments\n\nlink: A link function (IdentityLink, LogLink, or LogitLink)\nη: Linear predictor value(s)\n\nReturns\n\nThe transformed value(s) μ in the natural parameter space\n\nExamples\n\napply_invlink(LogLink(), 1.0)      # ≈ 2.718 (= exp(1))\napply_invlink(LogitLink(), 0.0)    # = 0.5   (= logistic(0))\napply_invlink(IdentityLink(), 1.5) # = 1.5\n\nSee also: apply_link, LinkFunction\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#Custom-AutoDiff-Models","page":"Observation Models","title":"Custom AutoDiff Models","text":"","category":"section"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.AutoDiffObservationModel","page":"Observation Models","title":"GaussianMarkovRandomFields.AutoDiffObservationModel","text":"AutoDiffObservationModel{F, B, SB, H} <: ObservationModel\n\nObservation model that uses automatic differentiation for a user-provided log-likelihood function.\n\nThis serves as a factory for creating AutoDiffLikelihood instances. The user provides a  log-likelihood function that can accept hyperparameters, and when materialized, creates a closure with the hyperparameters baked in.\n\nType Parameters\n\nF: Type of the log-likelihood function  \nB: Type of the AD backend for gradients\nSB: Type of the AD backend for Hessians\nH: Type of the hyperparameters tuple\n\nFields\n\nloglik_func::F: User-provided log-likelihood function with signature (x; kwargs...) -> Real\nn_latent::Int: Number of latent field components\ngrad_backend::B: AD backend for gradient computation\nhess_backend::SB: AD backend for Hessian computation\nhyperparams::H: Tuple of hyperparameter names that this model expects\n\nUsage\n\n# Define your log-likelihood function with hyperparameters\nfunction my_loglik(x; σ=1.0, y=[1.0, 2.0])\n    μ = x  # or some transformation of x\n    return -0.5 * sum((y .- μ).^2) / σ^2 - length(y) * log(σ)\nend\n\n# Create observation model specifying expected hyperparameters\nobs_model = AutoDiffObservationModel(my_loglik; n_latent=2, hyperparams=(:σ, :y))\n\n# Materialize with specific hyperparameters\nobs_lik = obs_model(σ=0.5, y=[1.2, 1.8])  # Creates AutoDiffLikelihood\n\n# Use normally\nll = loglik(x, obs_lik)\ngrad = loggrad(x, obs_lik)\nhess = loghessian(x, obs_lik)\n\nSee also: AutoDiffLikelihood, ObservationModel\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.AutoDiffLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.AutoDiffLikelihood","text":"AutoDiffLikelihood{F, B, SB, GP, HP} <: ObservationLikelihood\n\nAutomatic differentiation-based observation likelihood that wraps a user-provided log-likelihood function.\n\nThis is a materialized likelihood created from an AutoDiffObservationModel. The log-likelihood function is typically a closure that already includes hyperparameters and data.\n\nType Parameters\n\nF: Type of the log-likelihood function (usually a closure)\nB: Type of the AD backend for gradients\nSB: Type of the AD backend for Hessians\nGP: Type of the gradient preparation object\nHP: Type of the Hessian preparation object\n\nFields\n\nloglik_func::F: Log-likelihood function with signature (x) -> Real\ngrad_backend::B: AD backend for gradient computation\nhess_backend::SB: AD backend for Hessian computation\ngrad_prep::GP: Preparation object for gradient computation\nhess_prep::HP: Preparation object for Hessian computation\n\nUsage\n\nTypically created via AutoDiffObservationModel factory:\n\n# Define your log-likelihood function with hyperparameters\nfunction my_loglik(x; σ=1.0, y=[1.0, 2.0])\n    μ = x  # or some transformation of x\n    return -0.5 * sum((y .- μ).^2) / σ^2 - length(y) * log(σ)\nend\n\n# Create observation model\nobs_model = AutoDiffObservationModel(my_loglik, n_latent=2)\n\n# Materialize with data and hyperparameters\nobs_lik = obs_model(σ=0.5, y=[1.2, 1.8])  # Creates AutoDiffLikelihood\n\n# Use in the standard way\nx = [1.1, 1.9]\nll = loglik(x, obs_lik)\ngrad = loggrad(x, obs_lik)  # Uses prepared AD with optimal backends\nhess = loghessian(x, obs_lik)  # Automatically sparse when available!\n\nSparse Hessian Features\n\nThe Hessian computation automatically:\n\nDetects sparsity pattern using TracerSparsityDetector\nUses greedy coloring for efficient computation\nReturns a sparse matrix when beneficial\nFalls back to dense computation for small problems\n\nSee also: loglik, loggrad, loghessian\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#Advanced-Features-2","page":"Observation Models","title":"Advanced Features","text":"","category":"section"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.LinearlyTransformedObservationModel","page":"Observation Models","title":"GaussianMarkovRandomFields.LinearlyTransformedObservationModel","text":"LinearlyTransformedObservationModel{M, A} <: ObservationModel\n\nObservation model that applies a linear transformation to the latent field before  passing to a base observation model. This enables GLM-style modeling with design  matrices while maintaining full compatibility with existing observation models.\n\nMathematical Foundation\n\nThe wrapper transforms the full latent field x_full to linear predictors η via a  design matrix A:\n\nη = A * x_full  \nBase model operates on η as usual: p(y | η, θ)\nChain rule applied for gradients/Hessians: \n∇{xfull} ℓ = A^T ∇_η ℓ\n∇²{xfull} ℓ = A^T ∇²_η ℓ A\n\nType Parameters\n\nM <: ObservationModel: Type of the base observation model\nA: Type of the design matrix (typically AbstractMatrix)\n\nFields\n\nbase_model::M: The underlying observation model that operates on linear predictors\ndesign_matrix::A: Matrix mapping full latent field to observation-specific linear predictors\n\nUsage Pattern\n\n# Step 1: Create base observation model\nbase_model = ExponentialFamily(Poisson)  # LogLink by default\n\n# Step 2: Create design matrix (maps latent field to linear predictors)\n# For: y ~ intercept + temperature + group_effects\nA = [1.0  20.0  1.0  0.0  0.0;   # obs 1: intercept + temp + group1\n     1.0  25.0  1.0  0.0  0.0;   # obs 2: intercept + temp + group1  \n     1.0  30.0  0.0  1.0  0.0;   # obs 3: intercept + temp + group2\n     1.0  15.0  0.0  0.0  1.0]   # obs 4: intercept + temp + group3\n\n# Step 3: Create wrapped model\nobs_model = LinearlyTransformedObservationModel(base_model, A)\n\n# Step 4: Use in GMRF model - latent field now includes all components\n# x_full = [β₀, β₁, u₁, u₂, u₃]  # intercept, slope, group effects\n\n# Step 5: Materialize with data and hyperparameters\nobs_lik = obs_model(y; σ=1.2)  # Creates LinearlyTransformedLikelihood\n\n# Step 6: Fast evaluation in optimization loops\nll = loglik(x_full, obs_lik)\n\nHyperparameters\n\nAll hyperparameters come from the base observation model. The design matrix  introduces no new hyperparameters - it's a fixed linear transformation.\n\nSee also: LinearlyTransformedLikelihood, ExponentialFamily, ObservationModel\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.LinearlyTransformedLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.LinearlyTransformedLikelihood","text":"LinearlyTransformedLikelihood{L, A} <: ObservationLikelihood\n\nMaterialized likelihood for LinearlyTransformedObservationModel with precomputed  base likelihood and design matrix.\n\nThis is created by calling a LinearlyTransformedObservationModel instance with  data and hyperparameters, following the factory pattern used throughout the package.\n\nType Parameters\n\nL <: ObservationLikelihood: Type of the materialized base likelihood\nA: Type of the design matrix\n\nFields\n\nbase_likelihood::L: Materialized base observation likelihood (contains y and θ)\ndesign_matrix::A: Design matrix mapping full latent field to linear predictors\n\nUsage\n\nThis type is typically created automatically:\n\nltom = LinearlyTransformedObservationModel(base_model, design_matrix)\nltlik = ltom(y; σ=1.2)  # Creates LinearlyTransformedLikelihood\nll = loglik(x_full, ltlik)  # Fast evaluation\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.BinomialObservations","page":"Observation Models","title":"GaussianMarkovRandomFields.BinomialObservations","text":"BinomialObservations <: AbstractVector{Tuple{Int, Int}}\n\nCombined observation type for binomial data containing both successes and trials.\n\nThis type packages binomial observation data (number of successes and trials) into a single vector-like object where each element is a (successes, trials) tuple.\n\nFields\n\nsuccesses::Vector{Int}: Number of successes for each observation\ntrials::Vector{Int}: Number of trials for each observation\n\nExample\n\n# Create binomial observations\ny = BinomialObservations([3, 1, 4], [5, 8, 6])\n\n# Access as tuples\ny[1]  # (3, 5)\ny[2]  # (1, 8)\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.successes","page":"Observation Models","title":"GaussianMarkovRandomFields.successes","text":"successes(y::BinomialObservations) -> Vector{Int}\n\nExtract the successes vector from binomial observations.\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.trials","page":"Observation Models","title":"GaussianMarkovRandomFields.trials","text":"trials(y::BinomialObservations) -> Vector{Int}\n\nExtract the trials vector from binomial observations.\n\n\n\n\n\n","category":"function"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.CompositeObservations","page":"Observation Models","title":"GaussianMarkovRandomFields.CompositeObservations","text":"CompositeObservations{T<:Tuple} <: AbstractVector{Float64}\n\nA composite observation vector that stores observation data as a tuple of component vectors.\n\nThis type implements the AbstractVector interface and allows combining different observation datasets while maintaining their structure. The composite vector presents a unified view where indexing delegates to the appropriate component vector.\n\nFields\n\ncomponents::T: Tuple of observation vectors, one per likelihood component\n\nExample\n\ny1 = [1.0, 2.0, 3.0]  # Gaussian observations\ny2 = [4.0, 5.0]       # More Gaussian observations\ny_composite = CompositeObservations((y1, y2))\n\nlength(y_composite)    # 5\ny_composite[1]         # 1.0\ny_composite[4]         # 4.0\ncollect(y_composite)   # [1.0, 2.0, 3.0, 4.0, 5.0]\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.CompositeObservationModel","page":"Observation Models","title":"GaussianMarkovRandomFields.CompositeObservationModel","text":"CompositeObservationModel{T<:Tuple} <: ObservationModel\n\nAn observation model that combines multiple component observation models.\n\nThis type follows the factory pattern - it stores component observation models and  creates CompositeLikelihood instances when called with observation data and hyperparameters.\n\nFields\n\ncomponents::T: Tuple of component observation models for type stability\n\nExample\n\ngaussian_model = ExponentialFamily(Normal)\npoisson_model = ExponentialFamily(Poisson)\ncomposite_model = CompositeObservationModel((gaussian_model, poisson_model))\n\n# Materialize with data and hyperparameters\ny_composite = CompositeObservations(([1.0, 2.0], [3, 4]))\ncomposite_lik = composite_model(y_composite; σ=1.5)\n\n\n\n\n\n","category":"type"},{"location":"reference/observation_models/#GaussianMarkovRandomFields.CompositeLikelihood","page":"Observation Models","title":"GaussianMarkovRandomFields.CompositeLikelihood","text":"CompositeLikelihood{T<:Tuple} <: ObservationLikelihood\n\nA materialized composite likelihood that combines multiple component likelihoods.\n\nCreated by calling a CompositeObservationModel with observation data and hyperparameters. Provides efficient evaluation of log-likelihood, gradient, and Hessian by summing contributions from all component likelihoods.\n\nFields\n\ncomponents::T: Tuple of materialized component likelihoods\n\n\n\n\n\n","category":"type"},{"location":"tutorials/spatial_modelling_spdes/#Spatial-Modelling-with-SPDEs","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"","category":"section"},{"location":"tutorials/spatial_modelling_spdes/#Data-preprocessing","page":"Spatial Modelling with SPDEs","title":"Data preprocessing","text":"","category":"section"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"In the following, we are going to work with the meuse dataset. This dataset contains measurements of zinc concentrations in the soil near the Meuse river.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We begin by downloading the dataset.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"meuse_path = joinpath(@__DIR__, \"meuse.csv\")\nmeuse_URL = \"https://gist.githubusercontent.com/essicolo/91a2666f7c5972a91bca763daecdc5ff/raw/056bda04114d55b793469b2ab0097ec01a6d66c6/meuse.csv\"\ndownload(meuse_URL, meuse_path)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We load the CSV file into a DataFrame and inspect the first few rows.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"using CSV, DataFrames\ndf = DataFrame(CSV.File(meuse_path))\ndf[1:5, [:x, :y, :zinc]]","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"Let us visualize the data. We plot the zinc concentrations as a function of the x and y coordinates.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"using Plots\nx = convert(Vector{Float64}, df[:, :x])\ny = convert(Vector{Float64}, df[:, :y])\nzinc = df[:, :zinc]\nscatter(x, y, zcolor = zinc)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"Finally, in classic machine learning fashion, we split the data into a training and a test set. We use about 85% of the data for training and the remaining 15% for testing.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"using Random\ntrain_idcs = randsubseq(1:size(df, 1), 0.85)\ntest_idcs = [i for i in 1:size(df, 1) if isempty(searchsorted(train_idcs, i))]\nX = [x y]\nX_train = X[train_idcs, :]\nX_test = X[test_idcs, :]\ny_train = zinc[train_idcs]\ny_test = zinc[test_idcs]\nsize(X_train, 1), size(X_test, 1)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/#Spatial-Modelling","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling","text":"","category":"section"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"Matern Gaussian processes (GPs) are a powerful model class commonly used in geostatistics for such data. Unfortunately, without using any further tricks, GPs have a cubic runtime complexity. As the size of the dataset grows, this quickly becomes prohibitively expensive. In the tutorial on Autoregressive models, we learned that GMRFs enable highly efficient Gaussian inference through sparse precision matrices. Can we combine the modelling power of GPs with the efficiency of GMRFs?","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"Yes, we can: [1] told us how. It turns out that Matern processes may equivalently be interpreted as solutions of certain stochastic partial differential equations (SPDEs). If we discretize this SPDE appropriately – for example using the finite element method (FEM) – we get a discrete GMRF approximation of a Matern process. The approximation quality improves as the resolution of the FEM mesh increases. If this all sounds overly complicated to you, fear not! GaussianMarkovRandomFields.jl takes care of the technical details for you, so you can focus on the modelling.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We start by generating a FEM mesh for our data. Internally, GaussianMarkovRandomFields.jl computes a convex hull around the scattered data and then extends it slightly to counteract effects from the boundary condition of the SPDE.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"(Image: This image depicts the mesh generated for the scattered data.)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"The final output is a Ferrite.jl grid. We can also save the generated mesh, e.g. to visualize it via Gmsh.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"using GaussianMarkovRandomFields\npoints = zip(x, y)\ngrid = generate_mesh(points, 600.0, 100.0, save_path = \"meuse.msh\")","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We can now create a FEM discretization, which consists of the grid, a choice of basis functions, and a quadrature rule.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"using Ferrite\nip = Lagrange{RefTriangle, 1}()\nqr = QuadratureRule{RefTriangle}(2)\ndisc = FEMDiscretization(grid, ip, qr)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We now create a Matern SPDE and discretize it. While we could specify the Matern SPDE in terms of its direct parameters κ and ν, we here choose to specify it through the more easily interpretable parameters range and smoothness.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"spde = MaternSPDE{2}(range = 400.0, smoothness = 1)\nu_matern = discretize(spde, disc)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We can then condition the resulting Matern GMRF on the training data, where we assume an inverse noise variance of 10 (i.e. a variance of 0.1).","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"Λ_obs = 10.0\nA_train =\n    evaluation_matrix(disc, [Tensors.Vec(X_train[i, :]...) for i in 1:size(X_train, 1)])\nA_test = evaluation_matrix(disc, [Tensors.Vec(X_test[i, :]...) for i in 1:size(X_test, 1)])\nu_cond = condition_on_observations(u_matern, A_train, Λ_obs, y_train)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We can evaluate the RMSE of the posterior mean on the test data:","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"rmse = (a, b) -> sqrt(mean((a .- b) .^ 2))\nrmse(A_train * mean(u_cond), y_train), rmse(A_test * mean(u_cond), y_test)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We can also visualize the posterior mean and standard deviation. To this end, we write the corresponding vectors to a VTK file together with the grid data, which can then be visualized in e.g. Paraview.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"VTKGridFile(\"meuse_mean\", disc.dof_handler) do vtk\n    write_solution(vtk, disc.dof_handler, mean(u_cond))\nend\nusing Distributions\nVTKGridFile(\"meuse_std\", disc.dof_handler) do vtk\n    write_solution(vtk, disc.dof_handler, std(u_cond))\nend","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"In the end, our posterior mean looks like this: (Image: Mean)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"And the posterior standard deviation looks like this: (Image: Std)","category":"page"},{"location":"tutorials/spatial_modelling_spdes/#Final-note","page":"Spatial Modelling with SPDEs","title":"Final note","text":"","category":"section"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"We have seen how to combine the modelling power of GPs with the efficiency of GMRFs. This is a powerful combination that allows us to model spatial data efficiently and accurately. Note that these models are still sensitive to the choice of hyperparameters, i.e. the range and smoothness of the Matern process. So it's quite likely that you may find better hyperparameters than the ones used in this tutorial.","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"","category":"page"},{"location":"tutorials/spatial_modelling_spdes/","page":"Spatial Modelling with SPDEs","title":"Spatial Modelling with SPDEs","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/hard_constraints/#Hard-Constraints","page":"Hard Constraints","title":"Hard Constraints","text":"","category":"section"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Many applications require probabilistic models to satisfy hard equality constraints. For instance, in spatial statistics we might need mass conservation (sum of values equals zero), or in engineering applications we might require boundary conditions to be exactly satisfied. The ConstrainedGMRF type enables efficient Bayesian inference under linear equality constraints while preserving the computational advantages of sparse precision matrices.","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"A ConstrainedGMRF represents the conditional distribution x | Ax = e, where x follows an unconstrained GMRF prior and A, e define the linear constraints. Despite the resulting distribution being degenerate (the precision matrix becomes singular), all essential operations—sampling, mean computation, variance calculation, and further conditioning—remain computationally efficient through conditioning by Kriging.","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"The key insight is that while the constrained distribution has infinite precision in the constraint directions, it retains finite variance in the orthogonal complement, enabling meaningful inference.","category":"page"},{"location":"reference/hard_constraints/#Mathematical-Foundation","page":"Hard Constraints","title":"Mathematical Foundation","text":"","category":"section"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Given an unconstrained GMRF x ~ N(μ, Q⁻¹) and linear constraints Ax = e, the constrained distribution has:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Constrained mean:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"μ_c = μ - Q⁻¹A^T(AQ⁻¹A^T)⁻¹(Aμ - e)","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Constrained covariance:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Σ_c = Q⁻¹ - Q⁻¹A^T(AQ⁻¹A^T)⁻¹AQ⁻¹","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Sampling via Kriging:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"x_c = x - Q⁻¹A^T(AQ⁻¹A^T)⁻¹(Ax - e)","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"where x is drawn from the unconstrained distribution.","category":"page"},{"location":"reference/hard_constraints/#Basic-Usage-Pattern","page":"Hard Constraints","title":"Basic Usage Pattern","text":"","category":"section"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"# Step 1: Set up unconstrained prior GMRF\nprior_gmrf = GMRF(μ_prior, Q_prior)\n\n# Step 2: Define linear constraints Ax = e\nA = constraint_matrix  # m × n matrix\ne = constraint_vector  # m-dimensional vector\n\n# Step 3: Create constrained GMRF\nconstrained_gmrf = ConstrainedGMRF(prior_gmrf, A, e)\n\n# Step 4: Use like any other GMRF\nconstrained_sample = rand(constrained_gmrf)\nconstrained_mean = mean(constrained_gmrf)\nmarginal_vars = var(constrained_gmrf)","category":"page"},{"location":"reference/hard_constraints/#Examples","page":"Hard Constraints","title":"Examples","text":"","category":"section"},{"location":"reference/hard_constraints/#Sum-to-Zero-Constraint","page":"Hard Constraints","title":"Sum-to-Zero Constraint","text":"","category":"section"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"A common constraint in spatial modeling ensures mass conservation:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"using GaussianMarkovRandomFields, LinearAlgebra, SparseArrays\n\n# Prior: independent components\nn = 5\nprior_gmrf = GMRF(ones(n), spdiagm(0 => ones(n)))\n\n# Constraint: sum equals zero\nA = ones(1, n)  # 1×5 matrix [1 1 1 1 1]\ne = [0.0]       # sum = 0\n\n# Constrained GMRF\nconstrained_gmrf = ConstrainedGMRF(prior_gmrf, A, e)\n\n# Verify constraint satisfaction\nx = rand(constrained_gmrf)\n@assert abs(sum(x)) < 1e-10  # Constraint satisfied to numerical precision\n\n# Linear conditioning also works seamlessly\ny_obs = [0.3, -0.1]  # Observe first two components\nA_obs = sparse([1.0 0.0 0.0 0.0 0.0; 0.0 1.0 0.0 0.0 0.0])\nconditioned = linear_condition(constrained_gmrf; A=A_obs, Q_ϵ=10.0, y=y_obs)","category":"page"},{"location":"reference/hard_constraints/#Multiple-Constraints","page":"Hard Constraints","title":"Multiple Constraints","text":"","category":"section"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Complex applications often require multiple simultaneous constraints:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"# System with 4 variables\nn = 4\nprior_gmrf = GMRF(zeros(n), spdiagm(0 => ones(n)))\n\n# Two constraints:\n# 1. Sum equals zero: x₁ + x₂ + x₃ + x₄ = 0  \n# 2. Symmetry: x₁ = x₂\nA = [1.0 1.0 1.0 1.0;    # Sum constraint\n     1.0 -1.0 0.0 0.0]   # Symmetry constraint\ne = [0.0, 0.0]\n\nconstrained_gmrf = ConstrainedGMRF(prior_gmrf, A, e)\n\n# Both constraints are automatically satisfied\nx = rand(constrained_gmrf)\n@assert abs(sum(x)) < 1e-10           # Sum = 0\n@assert abs(x[1] - x[2]) < 1e-10      # x₁ = x₂","category":"page"},{"location":"reference/hard_constraints/#Boundary-Conditions-in-Spatial-Models","page":"Hard Constraints","title":"Boundary Conditions in Spatial Models","text":"","category":"section"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"In PDE-based spatial models, boundary conditions are naturally expressed as constraints:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"# Spatial grid with fixed boundary values\nn_interior = 16  # Interior points\nn_boundary = 8   # Boundary points  \nn_total = n_interior + n_boundary\n\n# Prior on interior + boundary points\nprior_gmrf = MaternSPDE(mesh, ν=1.5, κ=1.0)  # Some spatial prior\n\n# Constraint: fix boundary values\nA = [zeros(n_boundary, n_interior) I(n_boundary)]  # Select boundary points\ne = boundary_values  # Known boundary conditions\n\nconstrained_gmrf = ConstrainedGMRF(prior_gmrf, A, e)","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"Constraints are also commonly used for identifiability in hierarchical models (e.g., sum-to-zero constraints on random effects in INLA-style modeling).","category":"page"},{"location":"reference/hard_constraints/#Gaussian-Approximation-with-Constraints","page":"Hard Constraints","title":"Gaussian Approximation with Constraints","text":"","category":"section"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"The real power emerges when combining constrained priors with non-Gaussian observation models. For non-conjugate cases, the gaussian_approximation function uses Fisher scoring while automatically respecting constraints throughout optimization:","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"# Constrained prior for log-rates (sum-to-zero for identifiability)\nn = 6\nprior_gmrf = GMRF(zeros(n), spdiagm(0 => ones(n)))\nA = ones(1, n)\ne = [0.0]\nconstrained_prior = ConstrainedGMRF(prior_gmrf, A, e)\n\n# Poisson observations\nobs_model = ExponentialFamily(Poisson)\ny_counts = [5, 2, 8, 3, 6, 1]\nobs_lik = obs_model(y_counts)\n\n# Constrained Gaussian approximation to posterior\nconstrained_posterior = gaussian_approximation(constrained_prior, obs_lik)\n\n# All samples from posterior satisfy the constraint\nposterior_sample = rand(constrained_posterior)\n@assert abs(sum(posterior_sample)) < 1e-10","category":"page"},{"location":"reference/hard_constraints/","page":"Hard Constraints","title":"Hard Constraints","text":"For conjugate cases (Normal observations), specialized dispatches automatically use exact linear_condition instead of iterative optimization.","category":"page"},{"location":"reference/hard_constraints/#API-Reference","page":"Hard Constraints","title":"API Reference","text":"","category":"section"},{"location":"reference/hard_constraints/#GaussianMarkovRandomFields.ConstrainedGMRF","page":"Hard Constraints","title":"GaussianMarkovRandomFields.ConstrainedGMRF","text":"ConstrainedGMRF{T,L,G} <: AbstractGMRF{T,L}\n\nA Gaussian Markov Random Field with hard linear equality constraints.\n\nGiven an unconstrained GMRF x ~ N(μ, Q⁻¹) and constraints Ax = e, this represents the constrained distribution x | Ax = e.\n\nThis is a degenerate distribution (the precision matrix becomes singular), but sampling and mean computation are handled efficiently using conditioning by Kriging.\n\nMathematical Background\n\nFor x ~ N(μ, Q⁻¹) with constraint Ax = e, the constrained mean is:     μ_c = μ - Q⁻¹A^T(AQ⁻¹A^T)⁻¹(Aμ - e)\n\nAnd samples are obtained via:     x_c = x - Q⁻¹A^T(AQ⁻¹A^T)⁻¹(Ax - e)\n\nwhere x is a sample from the unconstrained distribution.\n\nThe constrained covariance matrix is:     Σ_c = Q⁻¹ - Q⁻¹A^T(AQ⁻¹A^T)⁻¹AQ⁻¹\n\nImplementation\n\nFor efficiency, the constructor precomputes:\n\nÃ^T = Q⁻¹A^T (via solving QL^T = A^T where Q = LL^T)\nL_c from Cholesky factorization of AÃ^T\nB = L^(-T)Ã^T L_c^(-T) for variance computations\n\nType Parameters\n\nT<:Real: The numeric type\nL<:Union{LinearMaps.LinearMap{T}, AbstractMatrix{T}}: The precision map type\nG<:AbstractGMRF{T,L}: The concrete type of the base GMRF\n\nFields\n\nbase_gmrf::G: The unconstrained GMRF\nconstraint_matrix::Matrix{T}: Constraint matrix A (converted to dense)\nconstraint_vector::Vector{T}: Constraint vector e\nA_tilde_T::Matrix{T}: Precomputed Q⁻¹A^T\nL_c::Cholesky{T, Matrix{T}}: Cholesky factorization of AÃ^T\nconstrained_mean::Vector{T}: Precomputed constrained mean\n\nConstructor\n\nConstrainedGMRF(base_gmrf::AbstractGMRF, A, e)\n\nCreate a constrained GMRF where base_gmrf is the unconstrained distribution, A is the constraint matrix, and e is the constraint vector such that Ax = e.\n\n\n\n\n\n","category":"type"},{"location":"dev-docs/solvers/#LinearSolve.jl-Integration","page":"Solvers","title":"LinearSolve.jl Integration","text":"","category":"section"},{"location":"dev-docs/solvers/#Overview","page":"Solvers","title":"Overview","text":"","category":"section"},{"location":"dev-docs/solvers/","page":"Solvers","title":"Solvers","text":"GaussianMarkovRandomFields.jl uses LinearSolve.jl as the backend for all linear algebra operations. This provides access to a wide range of solvers and preconditioners while maintaining a unified interface.","category":"page"},{"location":"dev-docs/solvers/#Key-Components","page":"Solvers","title":"Key Components","text":"","category":"section"},{"location":"dev-docs/solvers/","page":"Solvers","title":"Solvers","text":"The LinearSolve.jl integration provides several specialized modules:","category":"page"},{"location":"dev-docs/solvers/","page":"Solvers","title":"Solvers","text":"Selected Inversion: Efficient computation of diagonal elements of matrix inverses\nBackward Solve: Specialized solve operations for GMRF computations  \nLog Determinant: Efficient computation of log determinants for sparse matrices\nRBMC: Rao-Blackwellized Monte Carlo for marginal variance estimation","category":"page"},{"location":"dev-docs/solvers/#RBMC-Fallback","page":"Solvers","title":"RBMC Fallback","text":"","category":"section"},{"location":"dev-docs/solvers/","page":"Solvers","title":"Solvers","text":"When selected inversion is not available for a particular algorithm, the package automatically falls back to RBMC (Rao-Blackwellized Monte Carlo) for computing marginal variances:","category":"page"},{"location":"dev-docs/solvers/","page":"Solvers","title":"Solvers","text":"# RBMC automatically used when selected inversion unavailable\nvariance_vec = var(gmrf)  # Uses RBMC if needed","category":"page"},{"location":"reference/meshes/#Meshes","page":"Meshes","title":"Meshes","text":"","category":"section"},{"location":"reference/meshes/","page":"Meshes","title":"Meshes","text":"Finite element method discretizations of SPDEs require a mesh. GaussianMarkovRandomFields provides some utility functions to create meshes for common use cases.","category":"page"},{"location":"reference/meshes/","page":"Meshes","title":"Meshes","text":"For a hands-on example meshing a 2D point cloud, check out the tutorial Spatial Modelling with SPDEs.","category":"page"},{"location":"reference/meshes/#GaussianMarkovRandomFields.generate_mesh","page":"Meshes","title":"GaussianMarkovRandomFields.generate_mesh","text":"generate_mesh(mp::GeometryBasics.MultiPoint, buffer_width::Real,\n              interior_mesh_size::Real;\n              exterior_mesh_size::Real = 2 * interior_mesh_size,\n              element_order::Int = 1, save_path=nothing)\n\nGenerate a mesh for a spatial point cloud, with a buffer to counteract boundary effects from the SPDE discretization.\n\nInput\n\nmp – MultiPoint object\nbuffer_width – Width of the buffer around the convex hull\ninterior_mesh_size – Mesh size inside the convex hull\nexterior_mesh_size – Mesh size outside the convex hull\nelement_order – Order of the element basis functions\nsave_path – Optional path to save the mesh\n\nOutput\n\nA Ferrite.Grid object\n\nAlgorithm\n\nCreate the convex hull of the point cloud via LibGEOS\nCreate the buffer around the convex hull\nCreate a mesh for the buffered polygon using Gmsh\nTransfer the Gmsh information to Ferrite\n\n\n\n\n\ngenerate_mesh(points, buffer_width::Real, interior_mesh_size::Real;\n              exterior_mesh_size::Real=2 * interior_mesh_size,\n              element_order::Int=1, save_path=nothing)\n\nGenerate a mesh from a list of points using Gmsh.\n\nInput\n\npoints – List of points\nbuffer_width – Width of the buffer around the convex hull\ninterior_mesh_size – Mesh size inside the convex hull\nexterior_mesh_size – Mesh size outside the convex hull\nelement_order – Order of the elements\nsave_path – Path to save the mesh\n\nOutput\n\nA Ferrite.Grid object\n\n\n\n\n\n","category":"function"},{"location":"reference/meshes/#GaussianMarkovRandomFields.create_inflated_rectangle","page":"Meshes","title":"GaussianMarkovRandomFields.create_inflated_rectangle","text":"create_inflated_rectangle(x0, y0, dx, dy, boundary_width, interior_mesh_size,\nexterior_mesh_size = 2 * interior_mesh_size; element_order = 1)\n\nCreate a triangular FEM discretization of a rectangle with an inflated boundary. Useful for FEM discretizations of SPDEs, where the domain is often artificially inflated to avoid undesirable boundary effects. Mesh has physical groups \"Domain\", \"Interior\", \"Interior boundary\" and possibly \"Exterior boundary\".\n\nArguments\n\nx0::Real: x-coordinate of the bottom-left corner of the rectangle\ny0::Real: y-coordinate of the bottom-left corner of the rectangle\ndx::Real: Width of the rectangle\ndy::Real: Height of the rectangle\nboundary_width::Real: Width of the inflated boundary. If 0.0, mesh will not                         be inflated at all.\ninterior_mesh_size::Real: Mesh size in the interior of the rectangle\nexterior_mesh_size::Real: Mesh size in the exterior of the rectangle\nelement_order::Int: Order of the FEM elements\n\nReturns\n\ngrid::Ferrite.Grid: the FEM discretization of the rectangle\nboundary_tags::Vector{Int}: the indices of the boundary nodes\n\n\n\n\n\n","category":"function"},{"location":"bibliography/#Bibliography","page":"Bibliography","title":"Bibliography","text":"","category":"section"},{"location":"bibliography/","page":"Bibliography","title":"Bibliography","text":"F. Lindgren, H. Rue and J. Lindström. An Explicit Link between Gaussian Fields and Gaussian Markov Random Fields: The Stochastic Partial Differential Equation Approach. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 73, 423–498 (2011).\n\n\n\nL. Clarotto, D. Allard, T. Romary and N. Desassis. The SPDE Approach for Spatio-Temporal Datasets with Advection and Diffusion. Spatial Statistics 62, 100847 (2024).\n\n\n\nP. Sidén, F. Lindgren, D. Bolin and M. Villani. Efficient Covariance Approximations for Large Sparse Precision Matrices. Journal of Computational and Graphical Statistics 27, 898–909 (2018).\n\n\n\n","category":"page"},{"location":"#GaussianMarkovRandomFields.jl","page":"Home","title":"GaussianMarkovRandomFields.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Gaussian Markov Random Fields in Julia.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Gaussian Markov Random Fields (GMRFs) are Gaussian distributions with sparse precision (inverse covariance) matrices. GaussianMarkovRandomFields.jl provides utilities for working with GMRFs in Julia. The goal is to enable flexible and efficient Bayesian inference from GMRFs, powered by sparse linear algebra.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In particular, we support the creation of GMRFs through finite element method discretizations of stochastic partial differential equations (SPDEs). This unlocks efficient GMRF-based approximations to commonly used Gaussian process priors. Furthermore, the expressive power of SPDEs allows for flexible, problem-tailored priors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To get started with GaussianMarkovRandomFields.jl, consider going through the Tutorials.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GaussianMarkovRandomFields.jl can be installed via its GitHub repo from the Pkg REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/timweiland/GaussianMarkovRandomFields.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"Afterwards, you may load the package using","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussianMarkovRandomFields","category":"page"},{"location":"","page":"Home","title":"Home","text":"You're good to go!","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Info\nWhile a fair amount of time was spent on this documentation, it is far from perfect.  If you spot parts of the documentation that you find confusing or that are incomplete, please open an issue or a pull request. Your help is much appreciated!","category":"page"},{"location":"","page":"Home","title":"Home","text":"tip: Tip\nMissing a feature? Let us know! If you're interested in contributing, that's even better! Check our contribution guidelines for assistance.","category":"page"}]
}
